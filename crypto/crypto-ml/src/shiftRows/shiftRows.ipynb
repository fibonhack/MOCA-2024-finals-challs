{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = \"cuda\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "def rotate(word, n):\n",
    "    return word[n:]+word[0:n]\n",
    "def shiftRows(state):\n",
    "    ret = []\n",
    "    for i in range(4):\n",
    "        ret += rotate(state[i*4:i*4+4],i)\n",
    "    return ret\n",
    "def shiftRowsInv(state):\n",
    "    ret = []\n",
    "    for i in range(4):\n",
    "        ret += rotate(state[i*4:i*4+4],-i)\n",
    "    return ret\n",
    "\n",
    "state=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "state = shiftRows(state)\n",
    "assert state == [1, 2, 3, 4, 6, 7, 8, 5, 11, 12, 9, 10, 16, 13, 14, 15]\n",
    "state = shiftRowsInv(state)\n",
    "assert state == [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "noise = 0.0\n",
    "noise_n = 1\n",
    "\n",
    "def bin_list(Is):\n",
    "    ret = []\n",
    "    for i in Is:\n",
    "        b0 = torch.tensor([float(a) for a in bin(i)[2:].rjust(8,\"0\")], dtype=torch.float32)\n",
    "        # b0 += (torch.FloatTensor(8).uniform_(-noise, +noise))\n",
    "        b0 = b0.to(device)\n",
    "        ret = ret + [b0]\n",
    "    return ret\n",
    "\n",
    "train_data = []\n",
    "already = set()\n",
    "\n",
    "while len(train_data) < 10000:\n",
    "    Is = torch.randint(0,256,(16,))\n",
    "    if Is in already:\n",
    "        continue\n",
    "    already.add(Is)\n",
    "    for _ in range(noise_n):\n",
    "        b = bin_list(Is)\n",
    "        res = shiftRows(b)\n",
    "        b = torch.stack(b, dim=0).view(8*16)\n",
    "        res = torch.stack(res, dim=0).view(8*16)\n",
    "        train_data.append((b,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "         0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "         1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "         0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
       "         0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "         0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "         0., 1.], device='cuda:0'),\n",
       " tensor([0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "         0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
       "         1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "         0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "         0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 1.], device='cuda:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "def get_loaders(train_data, device=device):\n",
    "    test_size = 0.05\n",
    "    valid_size = 0.05\n",
    "    batch_size = 500\n",
    "    num_workers = 0\n",
    "\n",
    "    #cuda or cpu\n",
    "    device = torch.device(device)\n",
    "\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(test_size * num_train))\n",
    "    split2 = int(np.floor((valid_size+test_size) * num_train))\n",
    "    train_idx, valid_idx, test_idx = indices[split2:], indices[split:split2], indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    # prepare data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=test_sampler, num_workers=num_workers)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "train_loader, valid_loader, test_loader = get_loaders(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def noise_to_int(bits):\n",
    "    bits = [round(float(b)) for b in bits]\n",
    "    bits = \"\".join([str(b) if b in [0,1] else \"0\" if b<1/10**5 else \"1\" for b in bits])\n",
    "    return int(bits,2)\n",
    "\n",
    "def lin_to_list(t):\n",
    "    res = []\n",
    "    for i in range(len(t)//8):\n",
    "        res += [noise_to_int(t[i*8:i*8+8])]\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "X, Y = next(iter(train_loader))\n",
    "\n",
    "for x,y in zip(X,Y):\n",
    "    a = lin_to_list(x)\n",
    "    b = lin_to_list(y)\n",
    "    b1 = shiftRows(a)\n",
    "    assert b == b1\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, num_hidden, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = num_hidden\n",
    "        \n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        \n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "\n",
    "\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    # print(\"scores\",scores.shape)\n",
    "    if mask is not None:\n",
    "        # print(\"mask\",mask.shape)\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "    \n",
    "    # print(\"v\",v.shape)\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, num_hidden, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_hidden = num_hidden\n",
    "        self.d_k = num_hidden // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(num_hidden, num_hidden)\n",
    "        self.v_linear = nn.Linear(num_hidden, num_hidden)\n",
    "        self.k_linear = nn.Linear(num_hidden, num_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(num_hidden, num_hidden)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into N heads\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * N * sl * num_hidden\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.num_hidden)\n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_num, output_num, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "    \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(input_num, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, output_num)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ShiftRows(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShiftRows, self).__init__()\n",
    "        \n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(8*16,8*16),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(save_file, model, criterion, train_loader, valid_loader, optimizer=None, n_epochs = 100000, f=lin_to_list, lrate=0.005):\n",
    "    # number of epochs to train the model\n",
    "\n",
    "    if optimizer is None:\n",
    "        # specify optimizer (stochastic gradient descent) and learning rate = 0.001\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lrate)#, weight_decay=0.00000001)\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        results = 0\n",
    "        results_n = 0\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        i=0\n",
    "        for X, target in train_loader:\n",
    "            i+=1\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            target = target.to(device)\n",
    "            output = model(X)\n",
    "            # calculate the loss\n",
    "            # print(output)\n",
    "            # print(target)\n",
    "            loss = criterion(output, target) #\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*X.size(0)\n",
    "            if epoch%100 == 0:\n",
    "                for x,y in zip(output,target):\n",
    "                    # print(x.cpu().detach().numpy(),y)\n",
    "                    a = f(x.cpu().detach().numpy())\n",
    "                    # a = int(x[0])\n",
    "                    b = f(y.cpu().detach().numpy())\n",
    "                    # b = int(y[0])\n",
    "                    # a = noise_to_int(x)\n",
    "                    # b = noise_to_int(y)\n",
    "                    \n",
    "                    \n",
    "                    # print(a,b)\n",
    "                    # print(float(x[0]),float(y[0]))\n",
    "                    if a==b:\n",
    "                        \n",
    "                        results +=1\n",
    "                    results_n+=1\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for X, target in valid_loader:\n",
    "        \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(X)\n",
    "            # target = target.to(device)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # update running validation loss\n",
    "            valid_loss += loss.item()*X.size(0)\n",
    "            \n",
    "\n",
    "        # print training/validation statistics\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch+1,\n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss))\n",
    "            torch.save(model.state_dict(), save_file)\n",
    "            valid_loss_min = valid_loss\n",
    "            if train_loss <= 0.000000001:\n",
    "                print(\"stop: loss <= 0.00000\")\n",
    "                return\n",
    "            else:\n",
    "                print(\" loss >= 0.00000\")\n",
    "        \n",
    "        if results_n != 0 :\n",
    "            print(f\"{results/results_n=}\")\n",
    "            print(f\"{results}\")\n",
    "            # if results == results_n and valid_loss <= valid_loss_min:\n",
    "            #     print(\"stop: no errors\")\n",
    "            #     return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000151 \tValidation Loss: 0.000001\n",
      "Validation loss decreased (inf --> 0.000001).  Saving model ...\n",
      " loss >= 0.00000\n",
      "results/results_n=1.0\n",
      "9000\n",
      "Epoch: 2 \tTraining Loss: 0.000021 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000001 --> 0.000000).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 3 \tTraining Loss: 0.000004 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 4 \tTraining Loss: 0.000001 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 5 \tTraining Loss: 0.000000 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 6 \tTraining Loss: 0.000000 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 7 \tTraining Loss: 0.000000 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 8 \tTraining Loss: 0.000000 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "stop: loss <= 0.00000\n"
     ]
    }
   ],
   "source": [
    "# model = ShiftRows().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "train(\"shiftrows.pt\", model, criterion, train_loader, valid_loader, f=lin_to_list, lrate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ShiftRows().to(device)\n",
    "model.load_state_dict(torch.load(\"shiftrows.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/results_n=1.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "X, Y = next(iter(train_loader))\n",
    "\n",
    "# print(X,Y)\n",
    "results = 0\n",
    "results_n = 0\n",
    "O = model(X)\n",
    "for x,y in zip(O,Y):\n",
    "    a = lin_to_list(x.cpu().detach().numpy())\n",
    "    b = lin_to_list(y.cpu().detach().numpy())\n",
    "    # b = int(y[0]) pos_to_int\n",
    "    # b = noise_to_int(y)\n",
    "    # print(x,y)\n",
    "    # print(a,b)\n",
    "    if a==b:\n",
    "        \n",
    "        results +=1\n",
    "    results_n +=1\n",
    "print(f\"{results/results_n=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 1000 mix columns\n",
      "results/results_n=1.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "X, Y = next(iter(train_loader))\n",
    "\n",
    "# print(X,Y)\n",
    "results = 0\n",
    "results_n = 0\n",
    "O = X\n",
    "n = 1000\n",
    "for i in range(n):\n",
    "    O = model(O)\n",
    "for x,y in zip(X,O):\n",
    "    # print(x,y)\n",
    "    a = lin_to_list(x.cpu().detach().numpy())\n",
    "    b = lin_to_list(y.cpu().detach().numpy())\n",
    "    # b = int(y[0]) pos_to_int\n",
    "    # b = noise_to_int(y)\n",
    "    \n",
    "    r = a\n",
    "    for i in range(n):\n",
    "        r = shiftRows(r)\n",
    "    # print(r,b)\n",
    "    if r==b:\n",
    "        \n",
    "        results +=1\n",
    "    results_n +=1\n",
    "print(f\"after {n} mix columns\")\n",
    "print(f\"{results/results_n=}\")\n",
    "# print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "state = torch.load(\"shiftrows.pt\")\n",
    "for w,b in zip(state['body.0.weight'], state['body.0.bias']):\n",
    "    a = w+b\n",
    "    for i,v in enumerate(a):\n",
    "        if v > 0.9:\n",
    "            w[i] = 1.0\n",
    "        else:\n",
    "            w[i] = 0.0\n",
    "for i in range(len(state[\"body.0.bias\"])):\n",
    "    state[\"body.0.bias\"][i] = 0.0\n",
    "torch.save(state,\"shiftrows.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c4a0582a2f0c696ca3bf144d7a797081d9cfe83a50d52b918de14dcec4b2aff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
