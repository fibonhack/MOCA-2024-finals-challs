{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrcgg\\anaconda3\\envs\\ML-pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = \"cuda\"\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mode 2^32\n",
    "def add(a, b):\n",
    "    return (a + b) % 2**32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "noise = 0.0\n",
    "noise_n = 1\n",
    "\n",
    "train_data = []\n",
    "\n",
    "# every rotation is possible using only k=1\n",
    "\n",
    "while len(train_data) < 100000:\n",
    "    i = torch.randint(0, 2**32, (1,)).item()\n",
    "    j = torch.randint(0, 2**32, (1,)).item()\n",
    "    for _ in range(noise_n):\n",
    "        a = torch.tensor([float(a) for a in bin(i)[2:].rjust(32,\"0\")], dtype=torch.float32)\n",
    "        # a = torch.tensor([float(i)], dtype=torch.float32)\n",
    "        a = a.to(device)\n",
    "\n",
    "        b = torch.tensor([float(k) for k in bin(j)[2:].rjust(32,\"0\")], dtype=torch.float32)\n",
    "        # b = torch.tensor([float(j)], dtype=torch.float32)\n",
    "        b = b.to(device)\n",
    "\n",
    "        res = torch.tensor([float(k) for k in bin(add(i,j))[2:].rjust(32, \"0\")], dtype=torch.float32)\n",
    "        # res = torch.tensor([float(add(i,j))], dtype=torch.float32)\n",
    "        res = res.to(device)\n",
    "\n",
    "        # inp = torch.stack((a,b), dim=0).view(32*2)\n",
    "        # interleave a and b\n",
    "        inp = torch.zeros(32*2, dtype=torch.float32)\n",
    "        inp[0::2] = a\n",
    "        inp[1::2] = b\n",
    "        # reverse the order of the bits\n",
    "        inp = inp.view(32,2)\n",
    "        inp = inp.flip(0)\n",
    "        inp = inp.to(device)\n",
    "        train_data.append((inp,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 1.],\n",
       "         [0., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 0.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 0.],\n",
       "         [1., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 0.],\n",
       "         [1., 1.]], device='cuda:0'),\n",
       " tensor([0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
       "         0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for x,y in train_data:\n",
    "#     if x.shape != torch.Size([32+5]):\n",
    "#         print(x.shape)\n",
    "#         print(x,y)\n",
    "#         break\n",
    "#     if y.shape != torch.Size([32]):\n",
    "#         print(y.shape)\n",
    "#         print(x, y)\n",
    "#         break\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "def get_loaders(train_data, device=device):\n",
    "    test_size = 0.05\n",
    "    valid_size = 0.05\n",
    "    batch_size = 500\n",
    "    num_workers = 0\n",
    "\n",
    "    #cuda or cpu\n",
    "    device = torch.device(device)\n",
    "\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(test_size * num_train))\n",
    "    split2 = int(np.floor((valid_size+test_size) * num_train))\n",
    "    train_idx, valid_idx, test_idx = indices[split2:], indices[split:split2], indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    # prepare data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=test_sampler, num_workers=num_workers)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "train_loader, valid_loader, test_loader = get_loaders(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def noise_to_int(bits):\n",
    "    bits = [round(float(b)) for b in bits]\n",
    "    bits = \"\".join([str(b) if b in [0,1] else \"0\" if b<1/10**5 else \"1\" for b in bits])\n",
    "    return int(bits,2)\n",
    "\n",
    "def lin_to_tuple_32_32(t):\n",
    "    a = (t[:32],t[32:])\n",
    "    return tuple([noise_to_int(b) for b in a])\n",
    "\n",
    "def lin_to_list(t):\n",
    "    res = []\n",
    "    for i in range(len(t)//8):\n",
    "        res += [noise_to_int(t[i*8:i*8+8])]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "X, Y = next(iter(train_loader))\n",
    "\n",
    "for x,y in zip(X,Y):\n",
    "    a = torch.zeros(32)\n",
    "    b = torch.zeros(32)\n",
    "    for i in range(32):\n",
    "        a[31-i] = x[i][0]\n",
    "        b[31-i] = x[i][1]\n",
    "    a = noise_to_int(a)\n",
    "    b = noise_to_int(b)\n",
    "    res = noise_to_int(y)\n",
    "    # print(a,b)\n",
    "    # print(x)\n",
    "    # print(y)\n",
    "    b1 = add(a,b)\n",
    "    # print(b1, res)\n",
    "    assert res == b1\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# use RNN to learn the add function\n",
    "class Add(nn.Module):\n",
    "    def __init__(self):     \n",
    "        super(Add, self).__init__()\n",
    "        self.hidden_size = 2\n",
    "        self.num_layer = 2\n",
    "        self.input_size = 2\n",
    "        self.seq_len = 32\n",
    "        self.rnn = nn.LSTM(self.input_size, self.hidden_size, self.num_layer, batch_first=True, dtype=torch.float32)\n",
    "        self.fc = nn.Linear(self.hidden_size * self.seq_len, self.seq_len, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x, h = self.rnn(x)\n",
    "        # print(x.shape)\n",
    "        x = x.reshape(batch_size, self.hidden_size * self.seq_len)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(save_file, model, criterion, train_loader, valid_loader, optimizer=None, n_epochs = 100000, f=noise_to_int, lrate=0.005):\n",
    "    # number of epochs to train the model\n",
    "\n",
    "    if optimizer is None:\n",
    "        # specify optimizer (stochastic gradient descent) and learning rate = 0.001\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lrate)#, weight_decay=0.00000001)\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        results = 0\n",
    "        results_n = 0\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        i=0\n",
    "        for X, target in train_loader:\n",
    "            i+=1\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            target = target.to(device)\n",
    "            output = model(X)\n",
    "            # calculate the loss\n",
    "            # print(output)\n",
    "            # print(target)\n",
    "            loss = criterion(output, target) #\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*X.size(0)\n",
    "            if epoch%100 == 0:\n",
    "                for x,y in zip(output,target):\n",
    "                    # print(x.cpu().detach().numpy(),y)\n",
    "                    a = f(x.cpu().detach().numpy())\n",
    "                    # a = int(x[0])\n",
    "                    b = f(y.cpu().detach().numpy())\n",
    "                    # b = int(y[0])\n",
    "                    # a = noise_to_int(x)\n",
    "                    # b = noise_to_int(y)\n",
    "                    \n",
    "                    \n",
    "                    # print(a,b)\n",
    "                    # print(float(x[0]),float(y[0]))\n",
    "                    if a==b:\n",
    "                        \n",
    "                        results +=1\n",
    "                    results_n+=1\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for X, target in valid_loader:\n",
    "        \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(X)\n",
    "            # target = target.to(device)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # update running validation loss\n",
    "            valid_loss += loss.item()*X.size(0)\n",
    "            \n",
    "\n",
    "        # print training/validation statistics\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.14f} \\tValidation Loss: {:.14f}'.format(\n",
    "            epoch+1,\n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.14f} --> {:.14f}).  Saving model ...'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss))\n",
    "            torch.save(model.state_dict(), save_file)\n",
    "            valid_loss_min = valid_loss\n",
    "            if train_loss <= 0.0000000001:\n",
    "                print(\"stop: loss <= 0.00000\")\n",
    "                return\n",
    "            else:\n",
    "                print(\" loss >= 0.00000\")\n",
    "        \n",
    "        if results_n != 0 :\n",
    "            print(f\"{results/results_n=}\")\n",
    "            print(f\"{results}\")\n",
    "            # if results == results_n and valid_loss <= valid_loss_min:\n",
    "            #     print(\"stop: no errors\")\n",
    "            #     return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.61023167490959 \tValidation Loss: 0.02737148642540\n",
      "Validation loss decreased (inf --> 0.02737148642540).  Saving model ...\n",
      " loss >= 0.00000\n",
      "results/results_n=0.0\n",
      "0\n",
      "Epoch: 2 \tTraining Loss: 0.23317887801677 \tValidation Loss: 0.00633649788797\n",
      "Validation loss decreased (0.02737148642540 --> 0.00633649788797).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 3 \tTraining Loss: 0.06472138375044 \tValidation Loss: 0.00177221089602\n",
      "Validation loss decreased (0.00633649788797 --> 0.00177221089602).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 4 \tTraining Loss: 0.02025296614040 \tValidation Loss: 0.00072368561756\n",
      "Validation loss decreased (0.00177221089602 --> 0.00072368561756).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 5 \tTraining Loss: 0.00963618354406 \tValidation Loss: 0.00040468190797\n",
      "Validation loss decreased (0.00072368561756 --> 0.00040468190797).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 6 \tTraining Loss: 0.00583787360927 \tValidation Loss: 0.00026568951318\n",
      "Validation loss decreased (0.00040468190797 --> 0.00026568951318).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 7 \tTraining Loss: 0.00401749648619 \tValidation Loss: 0.00019107179134\n",
      "Validation loss decreased (0.00026568951318 --> 0.00019107179134).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 8 \tTraining Loss: 0.00297218455584 \tValidation Loss: 0.00014521440491\n",
      "Validation loss decreased (0.00019107179134 --> 0.00014521440491).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 9 \tTraining Loss: 0.00230157683254 \tValidation Loss: 0.00011453410611\n",
      "Validation loss decreased (0.00014521440491 --> 0.00011453410611).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 10 \tTraining Loss: 0.00183560125239 \tValidation Loss: 0.00009227423405\n",
      "Validation loss decreased (0.00011453410611 --> 0.00009227423405).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 11 \tTraining Loss: 0.00148900552886 \tValidation Loss: 0.00007544379623\n",
      "Validation loss decreased (0.00009227423405 --> 0.00007544379623).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 12 \tTraining Loss: 0.00122752079682 \tValidation Loss: 0.00006286876509\n",
      "Validation loss decreased (0.00007544379623 --> 0.00006286876509).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 13 \tTraining Loss: 0.00103076597152 \tValidation Loss: 0.00005311713729\n",
      "Validation loss decreased (0.00006286876509 --> 0.00005311713729).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 14 \tTraining Loss: 0.00087700335076 \tValidation Loss: 0.00004543986026\n",
      "Validation loss decreased (0.00005311713729 --> 0.00004543986026).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 15 \tTraining Loss: 0.00075359077746 \tValidation Loss: 0.00003923819109\n",
      "Validation loss decreased (0.00004543986026 --> 0.00003923819109).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 16 \tTraining Loss: 0.00065228131891 \tValidation Loss: 0.00003413452621\n",
      "Validation loss decreased (0.00003923819109 --> 0.00003413452621).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 17 \tTraining Loss: 0.00056996358267 \tValidation Loss: 0.00002988637018\n",
      "Validation loss decreased (0.00003413452621 --> 0.00002988637018).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 18 \tTraining Loss: 0.00049975724338 \tValidation Loss: 0.00002621730615\n",
      "Validation loss decreased (0.00002988637018 --> 0.00002621730615).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 19 \tTraining Loss: 0.00044030519042 \tValidation Loss: 0.00002317207633\n",
      "Validation loss decreased (0.00002621730615 --> 0.00002317207633).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 20 \tTraining Loss: 0.00038929479735 \tValidation Loss: 0.00002054226265\n",
      "Validation loss decreased (0.00002317207633 --> 0.00002054226265).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 21 \tTraining Loss: 0.00034702744641 \tValidation Loss: 0.00001827483371\n",
      "Validation loss decreased (0.00002054226265 --> 0.00001827483371).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 22 \tTraining Loss: 0.00030856432291 \tValidation Loss: 0.00001636506451\n",
      "Validation loss decreased (0.00001827483371 --> 0.00001636506451).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 23 \tTraining Loss: 0.00027542050011 \tValidation Loss: 0.00001473808225\n",
      "Validation loss decreased (0.00001636506451 --> 0.00001473808225).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 24 \tTraining Loss: 0.00024715844818 \tValidation Loss: 0.00001309657848\n",
      "Validation loss decreased (0.00001473808225 --> 0.00001309657848).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 25 \tTraining Loss: 0.00022215746561 \tValidation Loss: 0.00001185141933\n",
      "Validation loss decreased (0.00001309657848 --> 0.00001185141933).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 26 \tTraining Loss: 0.00019940635153 \tValidation Loss: 0.00001064934411\n",
      "Validation loss decreased (0.00001185141933 --> 0.00001064934411).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 27 \tTraining Loss: 0.00018155587532 \tValidation Loss: 0.00000977346252\n",
      "Validation loss decreased (0.00001064934411 --> 0.00000977346252).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 28 \tTraining Loss: 0.00016247997075 \tValidation Loss: 0.00000863593203\n",
      "Validation loss decreased (0.00000977346252 --> 0.00000863593203).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 29 \tTraining Loss: 0.00014679624190 \tValidation Loss: 0.00000781310402\n",
      "Validation loss decreased (0.00000863593203 --> 0.00000781310402).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 30 \tTraining Loss: 0.00013264258625 \tValidation Loss: 0.00000707747320\n",
      "Validation loss decreased (0.00000781310402 --> 0.00000707747320).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 31 \tTraining Loss: 0.00012050589779 \tValidation Loss: 0.00000659943045\n",
      "Validation loss decreased (0.00000707747320 --> 0.00000659943045).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 32 \tTraining Loss: 0.00010976740017 \tValidation Loss: 0.00000581982673\n",
      "Validation loss decreased (0.00000659943045 --> 0.00000581982673).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 33 \tTraining Loss: 0.00009897772350 \tValidation Loss: 0.00000533183651\n",
      "Validation loss decreased (0.00000581982673 --> 0.00000533183651).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 34 \tTraining Loss: 0.00009021759950 \tValidation Loss: 0.00000480032671\n",
      "Validation loss decreased (0.00000533183651 --> 0.00000480032671).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 35 \tTraining Loss: 0.00008166483767 \tValidation Loss: 0.00000437199229\n",
      "Validation loss decreased (0.00000480032671 --> 0.00000437199229).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 36 \tTraining Loss: 0.00007463861824 \tValidation Loss: 0.00000398235476\n",
      "Validation loss decreased (0.00000437199229 --> 0.00000398235476).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 37 \tTraining Loss: 0.00006755723069 \tValidation Loss: 0.00000362198985\n",
      "Validation loss decreased (0.00000398235476 --> 0.00000362198985).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 38 \tTraining Loss: 0.00006180304314 \tValidation Loss: 0.00000330510758\n",
      "Validation loss decreased (0.00000362198985 --> 0.00000330510758).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 39 \tTraining Loss: 0.00006057234519 \tValidation Loss: 0.00000490254792\n",
      "Epoch: 40 \tTraining Loss: 0.00044526089561 \tValidation Loss: 0.00001072278370\n",
      "Epoch: 41 \tTraining Loss: 0.00010602507675 \tValidation Loss: 0.00000406065850\n",
      "Epoch: 42 \tTraining Loss: 0.00006176867431 \tValidation Loss: 0.00000304097237\n",
      "Validation loss decreased (0.00000330510758 --> 0.00000304097237).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 43 \tTraining Loss: 0.00004863717782 \tValidation Loss: 0.00000248950962\n",
      "Validation loss decreased (0.00000304097237 --> 0.00000248950962).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 44 \tTraining Loss: 0.00004067763677 \tValidation Loss: 0.00000212093677\n",
      "Validation loss decreased (0.00000248950962 --> 0.00000212093677).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 45 \tTraining Loss: 0.00003508874939 \tValidation Loss: 0.00000185065819\n",
      "Validation loss decreased (0.00000212093677 --> 0.00000185065819).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 46 \tTraining Loss: 0.00003086180114 \tValidation Loss: 0.00000164171000\n",
      "Validation loss decreased (0.00000185065819 --> 0.00000164171000).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 47 \tTraining Loss: 0.00004915743300 \tValidation Loss: 0.00000221856801\n",
      "Epoch: 48 \tTraining Loss: 0.00003336939940 \tValidation Loss: 0.00000165010681\n",
      "Epoch: 49 \tTraining Loss: 0.00002657865779 \tValidation Loss: 0.00000137533506\n",
      "Validation loss decreased (0.00000164171000 --> 0.00000137533506).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 50 \tTraining Loss: 0.00002258531152 \tValidation Loss: 0.00000118845669\n",
      "Validation loss decreased (0.00000137533506 --> 0.00000118845669).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 51 \tTraining Loss: 0.00001971598610 \tValidation Loss: 0.00000104722787\n",
      "Validation loss decreased (0.00000118845669 --> 0.00000104722787).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 52 \tTraining Loss: 0.00001747612468 \tValidation Loss: 0.00000093349217\n",
      "Validation loss decreased (0.00000104722787 --> 0.00000093349217).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 53 \tTraining Loss: 0.00001565016950 \tValidation Loss: 0.00000083969655\n",
      "Validation loss decreased (0.00000093349217 --> 0.00000083969655).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 54 \tTraining Loss: 0.00001411455537 \tValidation Loss: 0.00000075913966\n",
      "Validation loss decreased (0.00000083969655 --> 0.00000075913966).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 55 \tTraining Loss: 0.00001279473431 \tValidation Loss: 0.00000069005471\n",
      "Validation loss decreased (0.00000075913966 --> 0.00000069005471).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 56 \tTraining Loss: 0.00001164329138 \tValidation Loss: 0.00000062894163\n",
      "Validation loss decreased (0.00000069005471 --> 0.00000062894163).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 57 \tTraining Loss: 0.00001062420105 \tValidation Loss: 0.00000057467988\n",
      "Validation loss decreased (0.00000062894163 --> 0.00000057467988).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 58 \tTraining Loss: 0.00000971472743 \tValidation Loss: 0.00000052595472\n",
      "Validation loss decreased (0.00000057467988 --> 0.00000052595472).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 59 \tTraining Loss: 0.00000889718617 \tValidation Loss: 0.00000048215306\n",
      "Validation loss decreased (0.00000052595472 --> 0.00000048215306).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 60 \tTraining Loss: 0.00000815581874 \tValidation Loss: 0.00000044243544\n",
      "Validation loss decreased (0.00000048215306 --> 0.00000044243544).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 61 \tTraining Loss: 0.00000748277463 \tValidation Loss: 0.00000040634998\n",
      "Validation loss decreased (0.00000044243544 --> 0.00000040634998).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 62 \tTraining Loss: 0.00000686869751 \tValidation Loss: 0.00000037278592\n",
      "Validation loss decreased (0.00000040634998 --> 0.00000037278592).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 63 \tTraining Loss: 0.00000630745614 \tValidation Loss: 0.00000034293237\n",
      "Validation loss decreased (0.00000037278592 --> 0.00000034293237).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 64 \tTraining Loss: 0.00000579331636 \tValidation Loss: 0.00000031498389\n",
      "Validation loss decreased (0.00000034293237 --> 0.00000031498389).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 65 \tTraining Loss: 0.00000532216286 \tValidation Loss: 0.00000028953683\n",
      "Validation loss decreased (0.00000031498389 --> 0.00000028953683).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 66 \tTraining Loss: 0.00000488924640 \tValidation Loss: 0.00000026599818\n",
      "Validation loss decreased (0.00000028953683 --> 0.00000026599818).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 67 \tTraining Loss: 0.00000449230690 \tValidation Loss: 0.00000024447656\n",
      "Validation loss decreased (0.00000026599818 --> 0.00000024447656).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 68 \tTraining Loss: 0.00000412727733 \tValidation Loss: 0.00000022467344\n",
      "Validation loss decreased (0.00000024447656 --> 0.00000022467344).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 69 \tTraining Loss: 0.00000379231336 \tValidation Loss: 0.00000020669185\n",
      "Validation loss decreased (0.00000022467344 --> 0.00000020669185).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 70 \tTraining Loss: 0.00000348457242 \tValidation Loss: 0.00000018981326\n",
      "Validation loss decreased (0.00000020669185 --> 0.00000018981326).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 71 \tTraining Loss: 0.00000320137620 \tValidation Loss: 0.00000017472753\n",
      "Validation loss decreased (0.00000018981326 --> 0.00000017472753).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 72 \tTraining Loss: 0.00000294204297 \tValidation Loss: 0.00000016058058\n",
      "Validation loss decreased (0.00000017472753 --> 0.00000016058058).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 73 \tTraining Loss: 0.00000270273201 \tValidation Loss: 0.00000014753832\n",
      "Validation loss decreased (0.00000016058058 --> 0.00000014753832).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 74 \tTraining Loss: 0.00000248340452 \tValidation Loss: 0.00000013543535\n",
      "Validation loss decreased (0.00000014753832 --> 0.00000013543535).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 75 \tTraining Loss: 0.00000228165765 \tValidation Loss: 0.00000012479537\n",
      "Validation loss decreased (0.00000013543535 --> 0.00000012479537).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 76 \tTraining Loss: 0.00000209705848 \tValidation Loss: 0.00000011458984\n",
      "Validation loss decreased (0.00000012479537 --> 0.00000011458984).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 77 \tTraining Loss: 0.00000192690083 \tValidation Loss: 0.00000010530740\n",
      "Validation loss decreased (0.00000011458984 --> 0.00000010530740).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 78 \tTraining Loss: 0.00000177121551 \tValidation Loss: 0.00000009683428\n",
      "Validation loss decreased (0.00000010530740 --> 0.00000009683428).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 79 \tTraining Loss: 0.00000162730948 \tValidation Loss: 0.00000008912662\n",
      "Validation loss decreased (0.00000009683428 --> 0.00000008912662).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 80 \tTraining Loss: 0.00000149576804 \tValidation Loss: 0.00000008193138\n",
      "Validation loss decreased (0.00000008912662 --> 0.00000008193138).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 81 \tTraining Loss: 0.00000137456625 \tValidation Loss: 0.00000007510953\n",
      "Validation loss decreased (0.00000008193138 --> 0.00000007510953).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 82 \tTraining Loss: 0.00000126341196 \tValidation Loss: 0.00000006914762\n",
      "Validation loss decreased (0.00000007510953 --> 0.00000006914762).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 83 \tTraining Loss: 0.00000116133480 \tValidation Loss: 0.00000006357099\n",
      "Validation loss decreased (0.00000006914762 --> 0.00000006357099).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 84 \tTraining Loss: 0.00000106731267 \tValidation Loss: 0.00000005846205\n",
      "Validation loss decreased (0.00000006357099 --> 0.00000005846205).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 85 \tTraining Loss: 0.00000098069053 \tValidation Loss: 0.00000005377551\n",
      "Validation loss decreased (0.00000005846205 --> 0.00000005377551).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 86 \tTraining Loss: 0.00000090162969 \tValidation Loss: 0.00000004953077\n",
      "Validation loss decreased (0.00000005377551 --> 0.00000004953077).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 87 \tTraining Loss: 0.00000082904142 \tValidation Loss: 0.00000004538001\n",
      "Validation loss decreased (0.00000004953077 --> 0.00000004538001).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 88 \tTraining Loss: 0.00000076174871 \tValidation Loss: 0.00000004180972\n",
      "Validation loss decreased (0.00000004538001 --> 0.00000004180972).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 89 \tTraining Loss: 0.00000070031029 \tValidation Loss: 0.00000003839714\n",
      "Validation loss decreased (0.00000004180972 --> 0.00000003839714).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 90 \tTraining Loss: 0.00000064386636 \tValidation Loss: 0.00000003540638\n",
      "Validation loss decreased (0.00000003839714 --> 0.00000003540638).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 91 \tTraining Loss: 0.00000059264928 \tValidation Loss: 0.00000003249747\n",
      "Validation loss decreased (0.00000003540638 --> 0.00000003249747).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 92 \tTraining Loss: 0.00000054464845 \tValidation Loss: 0.00000002993092\n",
      "Validation loss decreased (0.00000003249747 --> 0.00000002993092).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 93 \tTraining Loss: 0.00000050106117 \tValidation Loss: 0.00000002753687\n",
      "Validation loss decreased (0.00000002993092 --> 0.00000002753687).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 94 \tTraining Loss: 0.00000046132697 \tValidation Loss: 0.00000002527487\n",
      "Validation loss decreased (0.00000002753687 --> 0.00000002527487).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 95 \tTraining Loss: 0.00000042444239 \tValidation Loss: 0.00000002327513\n",
      "Validation loss decreased (0.00000002527487 --> 0.00000002327513).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 96 \tTraining Loss: 0.00000039074090 \tValidation Loss: 0.00000002146554\n",
      "Validation loss decreased (0.00000002327513 --> 0.00000002146554).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 97 \tTraining Loss: 0.00000035976414 \tValidation Loss: 0.00000001978148\n",
      "Validation loss decreased (0.00000002146554 --> 0.00000001978148).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 98 \tTraining Loss: 0.00000033160949 \tValidation Loss: 0.00000001819326\n",
      "Validation loss decreased (0.00000001978148 --> 0.00000001819326).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 99 \tTraining Loss: 0.00000030559792 \tValidation Loss: 0.00000001681812\n",
      "Validation loss decreased (0.00000001819326 --> 0.00000001681812).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 100 \tTraining Loss: 0.00000028167168 \tValidation Loss: 0.00000001552577\n",
      "Validation loss decreased (0.00000001681812 --> 0.00000001552577).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 101 \tTraining Loss: 0.00000025993414 \tValidation Loss: 0.00000001430284\n",
      "Validation loss decreased (0.00000001552577 --> 0.00000001430284).  Saving model ...\n",
      " loss >= 0.00000\n",
      "results/results_n=1.0\n",
      "90000\n",
      "Epoch: 102 \tTraining Loss: 0.00000023980460 \tValidation Loss: 0.00000001323654\n",
      "Validation loss decreased (0.00000001430284 --> 0.00000001323654).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 103 \tTraining Loss: 0.00000022149024 \tValidation Loss: 0.00000001219589\n",
      "Validation loss decreased (0.00000001323654 --> 0.00000001219589).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 104 \tTraining Loss: 0.00000020445856 \tValidation Loss: 0.00000001122975\n",
      "Validation loss decreased (0.00000001219589 --> 0.00000001122975).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 105 \tTraining Loss: 0.00000018900218 \tValidation Loss: 0.00000001043228\n",
      "Validation loss decreased (0.00000001122975 --> 0.00000001043228).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 106 \tTraining Loss: 0.00000017478474 \tValidation Loss: 0.00000000962319\n",
      "Validation loss decreased (0.00000001043228 --> 0.00000000962319).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 107 \tTraining Loss: 0.00000016180602 \tValidation Loss: 0.00000000890933\n",
      "Validation loss decreased (0.00000000962319 --> 0.00000000890933).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 108 \tTraining Loss: 0.00000014976787 \tValidation Loss: 0.00000000826229\n",
      "Validation loss decreased (0.00000000890933 --> 0.00000000826229).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 109 \tTraining Loss: 0.00000013877128 \tValidation Loss: 0.00000000770205\n",
      "Validation loss decreased (0.00000000826229 --> 0.00000000770205).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 110 \tTraining Loss: 0.00000012875983 \tValidation Loss: 0.00000000711579\n",
      "Validation loss decreased (0.00000000770205 --> 0.00000000711579).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 111 \tTraining Loss: 0.00000011949707 \tValidation Loss: 0.00000000662219\n",
      "Validation loss decreased (0.00000000711579 --> 0.00000000662219).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 112 \tTraining Loss: 0.00000011098911 \tValidation Loss: 0.00000000614897\n",
      "Validation loss decreased (0.00000000662219 --> 0.00000000614897).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 113 \tTraining Loss: 0.00000010312541 \tValidation Loss: 0.00000000569277\n",
      "Validation loss decreased (0.00000000614897 --> 0.00000000569277).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 114 \tTraining Loss: 0.00000009608992 \tValidation Loss: 0.00000000531377\n",
      "Validation loss decreased (0.00000000569277 --> 0.00000000531377).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 115 \tTraining Loss: 0.00000008932145 \tValidation Loss: 0.00000000494769\n",
      "Validation loss decreased (0.00000000531377 --> 0.00000000494769).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 116 \tTraining Loss: 0.00000008322966 \tValidation Loss: 0.00000000462778\n",
      "Validation loss decreased (0.00000000494769 --> 0.00000000462778).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 117 \tTraining Loss: 0.00000007761228 \tValidation Loss: 0.00000000431515\n",
      "Validation loss decreased (0.00000000462778 --> 0.00000000431515).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 118 \tTraining Loss: 0.00000007244311 \tValidation Loss: 0.00000000401990\n",
      "Validation loss decreased (0.00000000431515 --> 0.00000000401990).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 119 \tTraining Loss: 0.00000006768924 \tValidation Loss: 0.00000000376775\n",
      "Validation loss decreased (0.00000000401990 --> 0.00000000376775).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 120 \tTraining Loss: 0.00000006336112 \tValidation Loss: 0.00000000352902\n",
      "Validation loss decreased (0.00000000376775 --> 0.00000000352902).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 121 \tTraining Loss: 0.00000005925214 \tValidation Loss: 0.00000000329834\n",
      "Validation loss decreased (0.00000000352902 --> 0.00000000329834).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 122 \tTraining Loss: 0.00000005559534 \tValidation Loss: 0.00000000309366\n",
      "Validation loss decreased (0.00000000329834 --> 0.00000000309366).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 123 \tTraining Loss: 0.00000005212216 \tValidation Loss: 0.00000000290551\n",
      "Validation loss decreased (0.00000000309366 --> 0.00000000290551).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 124 \tTraining Loss: 0.00000004894652 \tValidation Loss: 0.00000000272146\n",
      "Validation loss decreased (0.00000000290551 --> 0.00000000272146).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 125 \tTraining Loss: 0.00000004602377 \tValidation Loss: 0.00000000255965\n",
      "Validation loss decreased (0.00000000272146 --> 0.00000000255965).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 126 \tTraining Loss: 0.00000004332698 \tValidation Loss: 0.00000000241152\n",
      "Validation loss decreased (0.00000000255965 --> 0.00000000241152).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 127 \tTraining Loss: 0.00000004089407 \tValidation Loss: 0.00000000230300\n",
      "Validation loss decreased (0.00000000241152 --> 0.00000000230300).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 128 \tTraining Loss: 0.00000003858533 \tValidation Loss: 0.00000000216846\n",
      "Validation loss decreased (0.00000000230300 --> 0.00000000216846).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 129 \tTraining Loss: 0.00000003644670 \tValidation Loss: 0.00000000204912\n",
      "Validation loss decreased (0.00000000216846 --> 0.00000000204912).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 130 \tTraining Loss: 0.00000003444107 \tValidation Loss: 0.00000000191842\n",
      "Validation loss decreased (0.00000000204912 --> 0.00000000191842).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 131 \tTraining Loss: 0.00000003260078 \tValidation Loss: 0.00000000181537\n",
      "Validation loss decreased (0.00000000191842 --> 0.00000000181537).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 132 \tTraining Loss: 0.00000003089748 \tValidation Loss: 0.00000000174627\n",
      "Validation loss decreased (0.00000000181537 --> 0.00000000174627).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 133 \tTraining Loss: 0.00000002924094 \tValidation Loss: 0.00000000163323\n",
      "Validation loss decreased (0.00000000174627 --> 0.00000000163323).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 134 \tTraining Loss: 0.00000002787166 \tValidation Loss: 0.00000000156349\n",
      "Validation loss decreased (0.00000000163323 --> 0.00000000156349).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 135 \tTraining Loss: 0.00000002645891 \tValidation Loss: 0.00000000148857\n",
      "Validation loss decreased (0.00000000156349 --> 0.00000000148857).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 136 \tTraining Loss: 0.00000002513641 \tValidation Loss: 0.00000000144454\n",
      "Validation loss decreased (0.00000000148857 --> 0.00000000144454).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 137 \tTraining Loss: 0.00000002399161 \tValidation Loss: 0.00000000133885\n",
      "Validation loss decreased (0.00000000144454 --> 0.00000000133885).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 138 \tTraining Loss: 0.00000002288318 \tValidation Loss: 0.00000000129528\n",
      "Validation loss decreased (0.00000000133885 --> 0.00000000129528).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 139 \tTraining Loss: 0.00000002183155 \tValidation Loss: 0.00000000122624\n",
      "Validation loss decreased (0.00000000129528 --> 0.00000000122624).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 140 \tTraining Loss: 0.00000002087443 \tValidation Loss: 0.00000000116699\n",
      "Validation loss decreased (0.00000000122624 --> 0.00000000116699).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 141 \tTraining Loss: 0.00000001994877 \tValidation Loss: 0.00000000111463\n",
      "Validation loss decreased (0.00000000116699 --> 0.00000000111463).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 142 \tTraining Loss: 0.00000001908785 \tValidation Loss: 0.00000000107143\n",
      "Validation loss decreased (0.00000000111463 --> 0.00000000107143).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 143 \tTraining Loss: 0.00000001830965 \tValidation Loss: 0.00000000103248\n",
      "Validation loss decreased (0.00000000107143 --> 0.00000000103248).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 144 \tTraining Loss: 0.00000001755534 \tValidation Loss: 0.00000000098653\n",
      "Validation loss decreased (0.00000000103248 --> 0.00000000098653).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 145 \tTraining Loss: 0.00000001684861 \tValidation Loss: 0.00000000094885\n",
      "Validation loss decreased (0.00000000098653 --> 0.00000000094885).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 146 \tTraining Loss: 0.00000001621520 \tValidation Loss: 0.00000000090871\n",
      "Validation loss decreased (0.00000000094885 --> 0.00000000090871).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 147 \tTraining Loss: 0.00000001557512 \tValidation Loss: 0.00000000087377\n",
      "Validation loss decreased (0.00000000090871 --> 0.00000000087377).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 148 \tTraining Loss: 0.00000001499358 \tValidation Loss: 0.00000000084154\n",
      "Validation loss decreased (0.00000000087377 --> 0.00000000084154).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 149 \tTraining Loss: 0.00000001445157 \tValidation Loss: 0.00000000082392\n",
      "Validation loss decreased (0.00000000084154 --> 0.00000000082392).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 150 \tTraining Loss: 0.00000001389104 \tValidation Loss: 0.00000000078254\n",
      "Validation loss decreased (0.00000000082392 --> 0.00000000078254).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 151 \tTraining Loss: 0.00000001353524 \tValidation Loss: 0.00000000076326\n",
      "Validation loss decreased (0.00000000078254 --> 0.00000000076326).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 152 \tTraining Loss: 0.00000001295612 \tValidation Loss: 0.00000000073984\n",
      "Validation loss decreased (0.00000000076326 --> 0.00000000073984).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 153 \tTraining Loss: 0.00000001263141 \tValidation Loss: 0.00000000071030\n",
      "Validation loss decreased (0.00000000073984 --> 0.00000000071030).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 154 \tTraining Loss: 0.00000001217075 \tValidation Loss: 0.00000000071693\n",
      "Epoch: 155 \tTraining Loss: 0.00000001189834 \tValidation Loss: 0.00000000066422\n",
      "Validation loss decreased (0.00000000071030 --> 0.00000000066422).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 156 \tTraining Loss: 0.00000001145956 \tValidation Loss: 0.00000000064196\n",
      "Validation loss decreased (0.00000000066422 --> 0.00000000064196).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 157 \tTraining Loss: 0.00000001103737 \tValidation Loss: 0.00000000062209\n",
      "Validation loss decreased (0.00000000064196 --> 0.00000000062209).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 158 \tTraining Loss: 0.00048598336857 \tValidation Loss: 0.00000061576428\n",
      "Epoch: 159 \tTraining Loss: 0.00001238720743 \tValidation Loss: 0.00000007731318\n",
      "Epoch: 160 \tTraining Loss: 0.00000113563045 \tValidation Loss: 0.00000004973024\n",
      "Epoch: 161 \tTraining Loss: 0.00000081249197 \tValidation Loss: 0.00000003810389\n",
      "Epoch: 162 \tTraining Loss: 0.00000064612468 \tValidation Loss: 0.00000003133065\n",
      "Epoch: 163 \tTraining Loss: 0.00000053826631 \tValidation Loss: 0.00000002657234\n",
      "Epoch: 164 \tTraining Loss: 0.00000046057250 \tValidation Loss: 0.00000002308221\n",
      "Epoch: 165 \tTraining Loss: 0.00000040118397 \tValidation Loss: 0.00000002030259\n",
      "Epoch: 166 \tTraining Loss: 0.00000035392499 \tValidation Loss: 0.00000001809280\n",
      "Epoch: 167 \tTraining Loss: 0.00000031524287 \tValidation Loss: 0.00000001624185\n",
      "Epoch: 168 \tTraining Loss: 0.00000028298996 \tValidation Loss: 0.00000001466626\n",
      "Epoch: 169 \tTraining Loss: 0.00000025553494 \tValidation Loss: 0.00000001332626\n",
      "Epoch: 170 \tTraining Loss: 0.00000023193297 \tValidation Loss: 0.00000001215943\n",
      "Epoch: 171 \tTraining Loss: 0.00000021137483 \tValidation Loss: 0.00000001113393\n",
      "Epoch: 172 \tTraining Loss: 0.00000019337230 \tValidation Loss: 0.00000001023086\n",
      "Epoch: 173 \tTraining Loss: 0.00000017740489 \tValidation Loss: 0.00000000942703\n",
      "Epoch: 174 \tTraining Loss: 0.00000016319772 \tValidation Loss: 0.00000000870751\n",
      "Epoch: 175 \tTraining Loss: 0.00000015049149 \tValidation Loss: 0.00000000806259\n",
      "Epoch: 176 \tTraining Loss: 0.00000013904308 \tValidation Loss: 0.00000000747538\n",
      "Epoch: 177 \tTraining Loss: 0.00000012871067 \tValidation Loss: 0.00000000693873\n",
      "Epoch: 178 \tTraining Loss: 0.00000011935509 \tValidation Loss: 0.00000000645820\n",
      "Epoch: 179 \tTraining Loss: 0.00000011079348 \tValidation Loss: 0.00000000601167\n",
      "Epoch: 180 \tTraining Loss: 0.00000010302267 \tValidation Loss: 0.00000000560795\n",
      "Epoch: 181 \tTraining Loss: 0.00000009589607 \tValidation Loss: 0.00000000523416\n",
      "Epoch: 182 \tTraining Loss: 0.00000008938020 \tValidation Loss: 0.00000000488915\n",
      "Epoch: 183 \tTraining Loss: 0.00000008334769 \tValidation Loss: 0.00000000457499\n",
      "Epoch: 184 \tTraining Loss: 0.00000007785609 \tValidation Loss: 0.00000000427908\n",
      "Epoch: 185 \tTraining Loss: 0.00000007274956 \tValidation Loss: 0.00000000400875\n",
      "Epoch: 186 \tTraining Loss: 0.00000006803616 \tValidation Loss: 0.00000000375580\n",
      "Epoch: 187 \tTraining Loss: 0.00000006367931 \tValidation Loss: 0.00000000352505\n",
      "Epoch: 188 \tTraining Loss: 0.00000005965202 \tValidation Loss: 0.00000000330829\n",
      "Epoch: 189 \tTraining Loss: 0.00000005591394 \tValidation Loss: 0.00000000310580\n",
      "Epoch: 190 \tTraining Loss: 0.00000005243459 \tValidation Loss: 0.00000000291970\n",
      "Epoch: 191 \tTraining Loss: 0.00000004920808 \tValidation Loss: 0.00000000274395\n",
      "Epoch: 192 \tTraining Loss: 0.00000004621409 \tValidation Loss: 0.00000000258216\n",
      "Epoch: 193 \tTraining Loss: 0.00000004342262 \tValidation Loss: 0.00000000243028\n",
      "Epoch: 194 \tTraining Loss: 0.00000004082663 \tValidation Loss: 0.00000000228823\n",
      "Epoch: 195 \tTraining Loss: 0.00000003840726 \tValidation Loss: 0.00000000215405\n",
      "Epoch: 196 \tTraining Loss: 0.00000003616043 \tValidation Loss: 0.00000000203208\n",
      "Epoch: 197 \tTraining Loss: 0.00000003405913 \tValidation Loss: 0.00000000191630\n",
      "Epoch: 198 \tTraining Loss: 0.00000003210293 \tValidation Loss: 0.00000000180847\n",
      "Epoch: 199 \tTraining Loss: 0.00000003026165 \tValidation Loss: 0.00000000170772\n",
      "Epoch: 200 \tTraining Loss: 0.00000002855162 \tValidation Loss: 0.00000000161283\n",
      "Epoch: 201 \tTraining Loss: 0.00000002695282 \tValidation Loss: 0.00000000152402\n",
      "results/results_n=1.0\n",
      "90000\n",
      "Epoch: 202 \tTraining Loss: 0.00000002545683 \tValidation Loss: 0.00000000144041\n",
      "Epoch: 203 \tTraining Loss: 0.00000002405985 \tValidation Loss: 0.00000000136190\n",
      "Epoch: 204 \tTraining Loss: 0.00000002275106 \tValidation Loss: 0.00000000128828\n",
      "Epoch: 205 \tTraining Loss: 0.00000002152864 \tValidation Loss: 0.00000000121884\n",
      "Epoch: 206 \tTraining Loss: 0.00000002038607 \tValidation Loss: 0.00000000115611\n",
      "Epoch: 207 \tTraining Loss: 0.00000001932041 \tValidation Loss: 0.00000000109593\n",
      "Epoch: 208 \tTraining Loss: 0.00000001830407 \tValidation Loss: 0.00000000103882\n",
      "Epoch: 209 \tTraining Loss: 0.00000001736262 \tValidation Loss: 0.00000000098348\n",
      "Epoch: 210 \tTraining Loss: 0.00000001647344 \tValidation Loss: 0.00000000093259\n",
      "Epoch: 211 \tTraining Loss: 0.00000001563866 \tValidation Loss: 0.00000000088710\n",
      "Epoch: 212 \tTraining Loss: 0.00000001485698 \tValidation Loss: 0.00000000084477\n",
      "Epoch: 213 \tTraining Loss: 0.00000001412490 \tValidation Loss: 0.00000000080388\n",
      "Epoch: 214 \tTraining Loss: 0.00000001344209 \tValidation Loss: 0.00000000076465\n",
      "Epoch: 215 \tTraining Loss: 0.00000001278532 \tValidation Loss: 0.00000000072997\n",
      "Epoch: 216 \tTraining Loss: 0.00000001218642 \tValidation Loss: 0.00000000069440\n",
      "Epoch: 217 \tTraining Loss: 0.00000001160761 \tValidation Loss: 0.00000000066049\n",
      "Epoch: 218 \tTraining Loss: 0.00000001106839 \tValidation Loss: 0.00000000063121\n",
      "Epoch: 219 \tTraining Loss: 0.00000001056110 \tValidation Loss: 0.00000000060286\n",
      "Validation loss decreased (0.00000000062209 --> 0.00000000060286).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 220 \tTraining Loss: 0.00000001008767 \tValidation Loss: 0.00000000057533\n",
      "Validation loss decreased (0.00000000060286 --> 0.00000000057533).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 221 \tTraining Loss: 0.00000000963843 \tValidation Loss: 0.00000000054855\n",
      "Validation loss decreased (0.00000000057533 --> 0.00000000054855).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 222 \tTraining Loss: 0.00000000922306 \tValidation Loss: 0.00000000052583\n",
      "Validation loss decreased (0.00000000054855 --> 0.00000000052583).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 223 \tTraining Loss: 0.00000000882798 \tValidation Loss: 0.00000000050277\n",
      "Validation loss decreased (0.00000000052583 --> 0.00000000050277).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 224 \tTraining Loss: 0.00000000845172 \tValidation Loss: 0.00000000048278\n",
      "Validation loss decreased (0.00000000050277 --> 0.00000000048278).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 225 \tTraining Loss: 0.00000000810855 \tValidation Loss: 0.00000000046095\n",
      "Validation loss decreased (0.00000000048278 --> 0.00000000046095).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 226 \tTraining Loss: 0.00000000776953 \tValidation Loss: 0.00000000044270\n",
      "Validation loss decreased (0.00000000046095 --> 0.00000000044270).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 227 \tTraining Loss: 0.00000000745681 \tValidation Loss: 0.00000000042595\n",
      "Validation loss decreased (0.00000000044270 --> 0.00000000042595).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 228 \tTraining Loss: 0.00000000716622 \tValidation Loss: 0.00000000040852\n",
      "Validation loss decreased (0.00000000042595 --> 0.00000000040852).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 229 \tTraining Loss: 0.00000000688367 \tValidation Loss: 0.00000000039324\n",
      "Validation loss decreased (0.00000000040852 --> 0.00000000039324).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 230 \tTraining Loss: 0.00000000662199 \tValidation Loss: 0.00000000037914\n",
      "Validation loss decreased (0.00000000039324 --> 0.00000000037914).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 231 \tTraining Loss: 0.00000000638655 \tValidation Loss: 0.00000000036344\n",
      "Validation loss decreased (0.00000000037914 --> 0.00000000036344).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 232 \tTraining Loss: 0.00000000615154 \tValidation Loss: 0.00000000035135\n",
      "Validation loss decreased (0.00000000036344 --> 0.00000000035135).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 233 \tTraining Loss: 0.00000000593365 \tValidation Loss: 0.00000000034094\n",
      "Validation loss decreased (0.00000000035135 --> 0.00000000034094).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 234 \tTraining Loss: 0.00000000573179 \tValidation Loss: 0.00000000032717\n",
      "Validation loss decreased (0.00000000034094 --> 0.00000000032717).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 235 \tTraining Loss: 0.00000000552981 \tValidation Loss: 0.00000000031510\n",
      "Validation loss decreased (0.00000000032717 --> 0.00000000031510).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 236 \tTraining Loss: 0.00000000533851 \tValidation Loss: 0.00000000030553\n",
      "Validation loss decreased (0.00000000031510 --> 0.00000000030553).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 237 \tTraining Loss: 0.00000000516484 \tValidation Loss: 0.00000000029601\n",
      "Validation loss decreased (0.00000000030553 --> 0.00000000029601).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 238 \tTraining Loss: 0.00000000500204 \tValidation Loss: 0.00000000028739\n",
      "Validation loss decreased (0.00000000029601 --> 0.00000000028739).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 239 \tTraining Loss: 0.00000000484402 \tValidation Loss: 0.00000000027882\n",
      "Validation loss decreased (0.00000000028739 --> 0.00000000027882).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 240 \tTraining Loss: 0.00000000469301 \tValidation Loss: 0.00000000026993\n",
      "Validation loss decreased (0.00000000027882 --> 0.00000000026993).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 241 \tTraining Loss: 0.00000000455125 \tValidation Loss: 0.00000000026209\n",
      "Validation loss decreased (0.00000000026993 --> 0.00000000026209).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 242 \tTraining Loss: 0.00000000442315 \tValidation Loss: 0.00000000025336\n",
      "Validation loss decreased (0.00000000026209 --> 0.00000000025336).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 243 \tTraining Loss: 0.00000000429118 \tValidation Loss: 0.00000000024850\n",
      "Validation loss decreased (0.00000000025336 --> 0.00000000024850).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 244 \tTraining Loss: 0.00000000426086 \tValidation Loss: 0.00000000025511\n",
      "Epoch: 245 \tTraining Loss: 0.00000000421897 \tValidation Loss: 0.00000000024188\n",
      "Validation loss decreased (0.00000000024850 --> 0.00000000024188).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 246 \tTraining Loss: 0.00000000405314 \tValidation Loss: 0.00000000023233\n",
      "Validation loss decreased (0.00000000024188 --> 0.00000000023233).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 247 \tTraining Loss: 0.00000000391383 \tValidation Loss: 0.00000000022611\n",
      "Validation loss decreased (0.00000000023233 --> 0.00000000022611).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 248 \tTraining Loss: 0.00000000379041 \tValidation Loss: 0.00000000021640\n",
      "Validation loss decreased (0.00000000022611 --> 0.00000000021640).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 249 \tTraining Loss: 0.00000000367522 \tValidation Loss: 0.00000000021052\n",
      "Validation loss decreased (0.00000000021640 --> 0.00000000021052).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 250 \tTraining Loss: 0.00000000356240 \tValidation Loss: 0.00000000020545\n",
      "Validation loss decreased (0.00000000021052 --> 0.00000000020545).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 251 \tTraining Loss: 0.00000000345625 \tValidation Loss: 0.00000000019932\n",
      "Validation loss decreased (0.00000000020545 --> 0.00000000019932).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 252 \tTraining Loss: 0.00000000336236 \tValidation Loss: 0.00000000019534\n",
      "Validation loss decreased (0.00000000019932 --> 0.00000000019534).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 253 \tTraining Loss: 0.00000000327505 \tValidation Loss: 0.00000000018856\n",
      "Validation loss decreased (0.00000000019534 --> 0.00000000018856).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 254 \tTraining Loss: 0.00000000318932 \tValidation Loss: 0.00000000018464\n",
      "Validation loss decreased (0.00000000018856 --> 0.00000000018464).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 255 \tTraining Loss: 0.00000000311159 \tValidation Loss: 0.00000000018198\n",
      "Validation loss decreased (0.00000000018464 --> 0.00000000018198).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 256 \tTraining Loss: 0.00000000305537 \tValidation Loss: 0.00000000017688\n",
      "Validation loss decreased (0.00000000018198 --> 0.00000000017688).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 257 \tTraining Loss: 0.00000000298783 \tValidation Loss: 0.00000000017501\n",
      "Validation loss decreased (0.00000000017688 --> 0.00000000017501).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 258 \tTraining Loss: 0.00000000291919 \tValidation Loss: 0.00000000016792\n",
      "Validation loss decreased (0.00000000017501 --> 0.00000000016792).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 259 \tTraining Loss: 0.00000000287675 \tValidation Loss: 0.00000000016823\n",
      "Epoch: 260 \tTraining Loss: 0.00000000280731 \tValidation Loss: 0.00000000016140\n",
      "Validation loss decreased (0.00000000016792 --> 0.00000000016140).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 261 \tTraining Loss: 0.00000000273084 \tValidation Loss: 0.00000000015898\n",
      "Validation loss decreased (0.00000000016140 --> 0.00000000015898).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 262 \tTraining Loss: 0.00000000267405 \tValidation Loss: 0.00000000015527\n",
      "Validation loss decreased (0.00000000015898 --> 0.00000000015527).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 263 \tTraining Loss: 0.00000000261028 \tValidation Loss: 0.00000000015400\n",
      "Validation loss decreased (0.00000000015527 --> 0.00000000015400).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 264 \tTraining Loss: 0.00000000256982 \tValidation Loss: 0.00000000014687\n",
      "Validation loss decreased (0.00000000015400 --> 0.00000000014687).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 265 \tTraining Loss: 0.00000000250485 \tValidation Loss: 0.00000000014517\n",
      "Validation loss decreased (0.00000000014687 --> 0.00000000014517).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 266 \tTraining Loss: 0.00000000246462 \tValidation Loss: 0.00000000014113\n",
      "Validation loss decreased (0.00000000014517 --> 0.00000000014113).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 267 \tTraining Loss: 0.00000000241498 \tValidation Loss: 0.00000000013711\n",
      "Validation loss decreased (0.00000000014113 --> 0.00000000013711).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 268 \tTraining Loss: 0.00000000236010 \tValidation Loss: 0.00000000013538\n",
      "Validation loss decreased (0.00000000013711 --> 0.00000000013538).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 269 \tTraining Loss: 0.00000000232225 \tValidation Loss: 0.00000000013534\n",
      "Validation loss decreased (0.00000000013538 --> 0.00000000013534).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 270 \tTraining Loss: 0.00000000227889 \tValidation Loss: 0.00000000013279\n",
      "Validation loss decreased (0.00000000013534 --> 0.00000000013279).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 271 \tTraining Loss: 0.00000000224203 \tValidation Loss: 0.00000000013299\n",
      "Epoch: 272 \tTraining Loss: 0.00000000220446 \tValidation Loss: 0.00000000012774\n",
      "Validation loss decreased (0.00000000013279 --> 0.00000000012774).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 273 \tTraining Loss: 0.00000000216911 \tValidation Loss: 0.00000000012228\n",
      "Validation loss decreased (0.00000000012774 --> 0.00000000012228).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 274 \tTraining Loss: 0.00000000213258 \tValidation Loss: 0.00000000012264\n",
      "Epoch: 275 \tTraining Loss: 0.00000000209767 \tValidation Loss: 0.00000000012113\n",
      "Validation loss decreased (0.00000000012228 --> 0.00000000012113).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 276 \tTraining Loss: 0.00000000206765 \tValidation Loss: 0.00000000011973\n",
      "Validation loss decreased (0.00000000012113 --> 0.00000000011973).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 277 \tTraining Loss: 0.00000000203498 \tValidation Loss: 0.00000000011720\n",
      "Validation loss decreased (0.00000000011973 --> 0.00000000011720).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 278 \tTraining Loss: 0.00000000200667 \tValidation Loss: 0.00000000011574\n",
      "Validation loss decreased (0.00000000011720 --> 0.00000000011574).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 279 \tTraining Loss: 0.00000000197634 \tValidation Loss: 0.00000000011362\n",
      "Validation loss decreased (0.00000000011574 --> 0.00000000011362).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 280 \tTraining Loss: 0.00000000193842 \tValidation Loss: 0.00000000011321\n",
      "Validation loss decreased (0.00000000011362 --> 0.00000000011321).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 281 \tTraining Loss: 0.00000000191623 \tValidation Loss: 0.00000000011189\n",
      "Validation loss decreased (0.00000000011321 --> 0.00000000011189).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 282 \tTraining Loss: 0.00000000188874 \tValidation Loss: 0.00000000010926\n",
      "Validation loss decreased (0.00000000011189 --> 0.00000000010926).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 283 \tTraining Loss: 0.00000000185642 \tValidation Loss: 0.00000000010939\n",
      "Epoch: 284 \tTraining Loss: 0.00000000182936 \tValidation Loss: 0.00000000010762\n",
      "Validation loss decreased (0.00000000010926 --> 0.00000000010762).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 285 \tTraining Loss: 0.00000000180980 \tValidation Loss: 0.00000000010381\n",
      "Validation loss decreased (0.00000000010762 --> 0.00000000010381).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 286 \tTraining Loss: 0.00000000177834 \tValidation Loss: 0.00000000010319\n",
      "Validation loss decreased (0.00000000010381 --> 0.00000000010319).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 287 \tTraining Loss: 0.00000000175139 \tValidation Loss: 0.00000000010129\n",
      "Validation loss decreased (0.00000000010319 --> 0.00000000010129).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 288 \tTraining Loss: 0.00000000173099 \tValidation Loss: 0.00000000010049\n",
      "Validation loss decreased (0.00000000010129 --> 0.00000000010049).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 289 \tTraining Loss: 0.00000000170717 \tValidation Loss: 0.00000000009840\n",
      "Validation loss decreased (0.00000000010049 --> 0.00000000009840).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 290 \tTraining Loss: 0.00000000168383 \tValidation Loss: 0.00000000009833\n",
      "Validation loss decreased (0.00000000009840 --> 0.00000000009833).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 291 \tTraining Loss: 0.00000000166042 \tValidation Loss: 0.00000000010123\n",
      "Epoch: 292 \tTraining Loss: 0.00000000164427 \tValidation Loss: 0.00000000009380\n",
      "Validation loss decreased (0.00000000009833 --> 0.00000000009380).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 293 \tTraining Loss: 0.00000000162106 \tValidation Loss: 0.00000000009393\n",
      "Epoch: 294 \tTraining Loss: 0.00000000160577 \tValidation Loss: 0.00000000009185\n",
      "Validation loss decreased (0.00000000009380 --> 0.00000000009185).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 295 \tTraining Loss: 0.00000000158472 \tValidation Loss: 0.00000000009255\n",
      "Epoch: 296 \tTraining Loss: 0.00000000157209 \tValidation Loss: 0.00000000008915\n",
      "Validation loss decreased (0.00000000009185 --> 0.00000000008915).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 297 \tTraining Loss: 0.00000000155041 \tValidation Loss: 0.00000000008916\n",
      "Epoch: 298 \tTraining Loss: 0.00000000153944 \tValidation Loss: 0.00000000008579\n",
      "Validation loss decreased (0.00000000008915 --> 0.00000000008579).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 299 \tTraining Loss: 0.00000000151927 \tValidation Loss: 0.00000000008814\n",
      "Epoch: 300 \tTraining Loss: 0.00000000150964 \tValidation Loss: 0.00000000008544\n",
      "Validation loss decreased (0.00000000008579 --> 0.00000000008544).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 301 \tTraining Loss: 0.00000000149546 \tValidation Loss: 0.00000000008397\n",
      "Validation loss decreased (0.00000000008544 --> 0.00000000008397).  Saving model ...\n",
      " loss >= 0.00000\n",
      "results/results_n=1.0\n",
      "90000\n",
      "Epoch: 302 \tTraining Loss: 0.00000000147728 \tValidation Loss: 0.00000000008516\n",
      "Epoch: 303 \tTraining Loss: 0.00000000146991 \tValidation Loss: 0.00000000008220\n",
      "Validation loss decreased (0.00000000008397 --> 0.00000000008220).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 304 \tTraining Loss: 0.00000000145320 \tValidation Loss: 0.00000000008453\n",
      "Epoch: 305 \tTraining Loss: 0.00000000144409 \tValidation Loss: 0.00000000008320\n",
      "Epoch: 306 \tTraining Loss: 0.00000000143055 \tValidation Loss: 0.00000000008134\n",
      "Validation loss decreased (0.00000000008220 --> 0.00000000008134).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 307 \tTraining Loss: 0.00000000141705 \tValidation Loss: 0.00000000007922\n",
      "Validation loss decreased (0.00000000008134 --> 0.00000000007922).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 308 \tTraining Loss: 0.00000000140576 \tValidation Loss: 0.00000000008088\n",
      "Epoch: 309 \tTraining Loss: 0.00000000140216 \tValidation Loss: 0.00000000007762\n",
      "Validation loss decreased (0.00000000007922 --> 0.00000000007762).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 310 \tTraining Loss: 0.00000000138581 \tValidation Loss: 0.00000000007695\n",
      "Validation loss decreased (0.00000000007762 --> 0.00000000007695).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 311 \tTraining Loss: 0.00000000137067 \tValidation Loss: 0.00000000007758\n",
      "Epoch: 312 \tTraining Loss: 0.00000000136405 \tValidation Loss: 0.00000000007721\n",
      "Epoch: 313 \tTraining Loss: 0.00000000136942 \tValidation Loss: 0.00000000007646\n",
      "Validation loss decreased (0.00000000007695 --> 0.00000000007646).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 314 \tTraining Loss: 0.00000000134662 \tValidation Loss: 0.00000000007953\n",
      "Epoch: 315 \tTraining Loss: 0.00000000133431 \tValidation Loss: 0.00000000007564\n",
      "Validation loss decreased (0.00000000007646 --> 0.00000000007564).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 316 \tTraining Loss: 0.00000000132419 \tValidation Loss: 0.00000000007322\n",
      "Validation loss decreased (0.00000000007564 --> 0.00000000007322).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 317 \tTraining Loss: 0.00031789117227 \tValidation Loss: 0.00000028930082\n",
      "Epoch: 318 \tTraining Loss: 0.00011337018415 \tValidation Loss: 0.00000004148720\n",
      "Epoch: 319 \tTraining Loss: 0.00000063160208 \tValidation Loss: 0.00000002834595\n",
      "Epoch: 320 \tTraining Loss: 0.00000045371210 \tValidation Loss: 0.00000002182356\n",
      "Epoch: 321 \tTraining Loss: 0.00000035589132 \tValidation Loss: 0.00000001769789\n",
      "Epoch: 322 \tTraining Loss: 0.00000029172780 \tValidation Loss: 0.00000001481774\n",
      "Epoch: 323 \tTraining Loss: 0.00000024578216 \tValidation Loss: 0.00000001268739\n",
      "Epoch: 324 \tTraining Loss: 0.00000021101733 \tValidation Loss: 0.00000001100968\n",
      "Epoch: 325 \tTraining Loss: 0.00000018372004 \tValidation Loss: 0.00000000967228\n",
      "Epoch: 326 \tTraining Loss: 0.00000016171478 \tValidation Loss: 0.00000000857271\n",
      "Epoch: 327 \tTraining Loss: 0.00000014360718 \tValidation Loss: 0.00000000765689\n",
      "Epoch: 328 \tTraining Loss: 0.00000012843034 \tValidation Loss: 0.00000000688698\n",
      "Epoch: 329 \tTraining Loss: 0.00000011559607 \tValidation Loss: 0.00000000622060\n",
      "Epoch: 330 \tTraining Loss: 0.00000010462261 \tValidation Loss: 0.00000000565147\n",
      "Epoch: 331 \tTraining Loss: 0.00000009513604 \tValidation Loss: 0.00000000515442\n",
      "Epoch: 332 \tTraining Loss: 0.00000008691598 \tValidation Loss: 0.00000000472091\n",
      "Epoch: 333 \tTraining Loss: 0.00000007972003 \tValidation Loss: 0.00000000433610\n",
      "Epoch: 334 \tTraining Loss: 0.00000007337089 \tValidation Loss: 0.00000000399942\n",
      "Epoch: 335 \tTraining Loss: 0.00000006775559 \tValidation Loss: 0.00000000369654\n",
      "Epoch: 336 \tTraining Loss: 0.00000006275610 \tValidation Loss: 0.00000000342731\n",
      "Epoch: 337 \tTraining Loss: 0.00000005827075 \tValidation Loss: 0.00000000318622\n",
      "Epoch: 338 \tTraining Loss: 0.00000005423644 \tValidation Loss: 0.00000000296592\n",
      "Epoch: 339 \tTraining Loss: 0.00000005057428 \tValidation Loss: 0.00000000276812\n",
      "Epoch: 340 \tTraining Loss: 0.00000004722386 \tValidation Loss: 0.00000000258552\n",
      "Epoch: 341 \tTraining Loss: 0.00000004415440 \tValidation Loss: 0.00000000241652\n",
      "Epoch: 342 \tTraining Loss: 0.00000004131620 \tValidation Loss: 0.00000000226455\n",
      "Epoch: 343 \tTraining Loss: 0.00000003870359 \tValidation Loss: 0.00000000212205\n",
      "Epoch: 344 \tTraining Loss: 0.00000003627151 \tValidation Loss: 0.00000000198716\n",
      "Epoch: 345 \tTraining Loss: 0.00000003400323 \tValidation Loss: 0.00000000186450\n",
      "Epoch: 346 \tTraining Loss: 0.00000003189305 \tValidation Loss: 0.00000000175147\n",
      "Epoch: 347 \tTraining Loss: 0.00000002991081 \tValidation Loss: 0.00000000164256\n",
      "Epoch: 348 \tTraining Loss: 0.00000002805761 \tValidation Loss: 0.00000000154168\n",
      "Epoch: 349 \tTraining Loss: 0.00000002632919 \tValidation Loss: 0.00000000144903\n",
      "Epoch: 350 \tTraining Loss: 0.00000002470653 \tValidation Loss: 0.00000000135869\n",
      "Epoch: 351 \tTraining Loss: 0.00000002318116 \tValidation Loss: 0.00000000127756\n",
      "Epoch: 352 \tTraining Loss: 0.00000002175865 \tValidation Loss: 0.00000000119884\n",
      "Epoch: 353 \tTraining Loss: 0.00000002042389 \tValidation Loss: 0.00000000112551\n",
      "Epoch: 354 \tTraining Loss: 0.00000001916768 \tValidation Loss: 0.00000000105804\n",
      "Epoch: 355 \tTraining Loss: 0.00000001798800 \tValidation Loss: 0.00000000099275\n",
      "Epoch: 356 \tTraining Loss: 0.00000001689449 \tValidation Loss: 0.00000000093185\n",
      "Epoch: 357 \tTraining Loss: 0.00000001586247 \tValidation Loss: 0.00000000087576\n",
      "Epoch: 358 \tTraining Loss: 0.00000001490167 \tValidation Loss: 0.00000000082389\n",
      "Epoch: 359 \tTraining Loss: 0.00000001399166 \tValidation Loss: 0.00000000077572\n",
      "Epoch: 360 \tTraining Loss: 0.00000001315300 \tValidation Loss: 0.00000000072947\n",
      "Epoch: 361 \tTraining Loss: 0.00000001237769 \tValidation Loss: 0.00000000068417\n",
      "Epoch: 362 \tTraining Loss: 0.00000001163935 \tValidation Loss: 0.00000000064435\n",
      "Epoch: 363 \tTraining Loss: 0.00000001095110 \tValidation Loss: 0.00000000060685\n",
      "Epoch: 364 \tTraining Loss: 0.00000001031447 \tValidation Loss: 0.00000000057194\n",
      "Epoch: 365 \tTraining Loss: 0.00000000971698 \tValidation Loss: 0.00000000053808\n",
      "Epoch: 366 \tTraining Loss: 0.00000000916303 \tValidation Loss: 0.00000000050828\n",
      "Epoch: 367 \tTraining Loss: 0.00000000864004 \tValidation Loss: 0.00000000047958\n",
      "Epoch: 368 \tTraining Loss: 0.00000000816428 \tValidation Loss: 0.00000000045307\n",
      "Epoch: 369 \tTraining Loss: 0.00000000771965 \tValidation Loss: 0.00000000042990\n",
      "Epoch: 370 \tTraining Loss: 0.00000000730027 \tValidation Loss: 0.00000000040716\n",
      "Epoch: 371 \tTraining Loss: 0.00000000692163 \tValidation Loss: 0.00000000038577\n",
      "Epoch: 372 \tTraining Loss: 0.00000000656844 \tValidation Loss: 0.00000000036581\n",
      "Epoch: 373 \tTraining Loss: 0.00000000624666 \tValidation Loss: 0.00000000034800\n",
      "Epoch: 374 \tTraining Loss: 0.00000000594596 \tValidation Loss: 0.00000000033077\n",
      "Epoch: 375 \tTraining Loss: 0.00000000566952 \tValidation Loss: 0.00000000031523\n",
      "Epoch: 376 \tTraining Loss: 0.00000000541358 \tValidation Loss: 0.00000000030201\n",
      "Epoch: 377 \tTraining Loss: 0.00000000518052 \tValidation Loss: 0.00000000028873\n",
      "Epoch: 378 \tTraining Loss: 0.00000000496013 \tValidation Loss: 0.00000000027506\n",
      "Epoch: 379 \tTraining Loss: 0.00000000475319 \tValidation Loss: 0.00000000026356\n",
      "Epoch: 380 \tTraining Loss: 0.00000000454892 \tValidation Loss: 0.00000000025213\n",
      "Epoch: 381 \tTraining Loss: 0.00000000436252 \tValidation Loss: 0.00000000024145\n",
      "Epoch: 382 \tTraining Loss: 0.00000000418423 \tValidation Loss: 0.00000000023108\n",
      "Epoch: 383 \tTraining Loss: 0.00000000401384 \tValidation Loss: 0.00000000022199\n",
      "Epoch: 384 \tTraining Loss: 0.00000000385003 \tValidation Loss: 0.00000000021331\n",
      "Epoch: 385 \tTraining Loss: 0.00000000370043 \tValidation Loss: 0.00000000020497\n",
      "Epoch: 386 \tTraining Loss: 0.00000000355821 \tValidation Loss: 0.00000000019653\n",
      "Epoch: 387 \tTraining Loss: 0.00000000341095 \tValidation Loss: 0.00000000018941\n",
      "Epoch: 388 \tTraining Loss: 0.00000000328427 \tValidation Loss: 0.00000000018194\n",
      "Epoch: 389 \tTraining Loss: 0.00000000316616 \tValidation Loss: 0.00000000017459\n",
      "Epoch: 390 \tTraining Loss: 0.00000000304582 \tValidation Loss: 0.00000000016795\n",
      "Epoch: 391 \tTraining Loss: 0.00000000293376 \tValidation Loss: 0.00000000016162\n",
      "Epoch: 392 \tTraining Loss: 0.00000000282917 \tValidation Loss: 0.00000000015609\n",
      "Epoch: 393 \tTraining Loss: 0.00000000273181 \tValidation Loss: 0.00000000015102\n",
      "Epoch: 394 \tTraining Loss: 0.00000000263602 \tValidation Loss: 0.00000000014562\n",
      "Epoch: 395 \tTraining Loss: 0.00000000254873 \tValidation Loss: 0.00000000014028\n",
      "Epoch: 396 \tTraining Loss: 0.00000000246333 \tValidation Loss: 0.00000000013504\n",
      "Epoch: 397 \tTraining Loss: 0.00000000238704 \tValidation Loss: 0.00000000013091\n",
      "Epoch: 398 \tTraining Loss: 0.00000000229826 \tValidation Loss: 0.00000000012759\n",
      "Epoch: 399 \tTraining Loss: 0.00000000223484 \tValidation Loss: 0.00000000012370\n",
      "Epoch: 400 \tTraining Loss: 0.00000000216428 \tValidation Loss: 0.00000000011992\n",
      "Epoch: 401 \tTraining Loss: 0.00000000210509 \tValidation Loss: 0.00000000011586\n",
      "results/results_n=1.0\n",
      "90000\n",
      "Epoch: 402 \tTraining Loss: 0.00000000204003 \tValidation Loss: 0.00000000011323\n",
      "Epoch: 403 \tTraining Loss: 0.00000000198299 \tValidation Loss: 0.00000000011064\n",
      "Epoch: 404 \tTraining Loss: 0.00000000193516 \tValidation Loss: 0.00000000010699\n",
      "Epoch: 405 \tTraining Loss: 0.00000000188176 \tValidation Loss: 0.00000000010392\n",
      "Epoch: 406 \tTraining Loss: 0.00000000183424 \tValidation Loss: 0.00000000010183\n",
      "Epoch: 407 \tTraining Loss: 0.00000000178257 \tValidation Loss: 0.00000000009948\n",
      "Epoch: 408 \tTraining Loss: 0.00000000174794 \tValidation Loss: 0.00000000009604\n",
      "Epoch: 409 \tTraining Loss: 0.00000000169715 \tValidation Loss: 0.00000000009494\n",
      "Epoch: 410 \tTraining Loss: 0.00000000166237 \tValidation Loss: 0.00000000009295\n",
      "Epoch: 411 \tTraining Loss: 0.00000000162508 \tValidation Loss: 0.00000000008984\n",
      "Epoch: 412 \tTraining Loss: 0.00000000158992 \tValidation Loss: 0.00000000008808\n",
      "Epoch: 413 \tTraining Loss: 0.00000000155196 \tValidation Loss: 0.00000000008700\n",
      "Epoch: 414 \tTraining Loss: 0.00000000152228 \tValidation Loss: 0.00000000008447\n",
      "Epoch: 415 \tTraining Loss: 0.00000000149047 \tValidation Loss: 0.00000000008261\n",
      "Epoch: 416 \tTraining Loss: 0.00000000146268 \tValidation Loss: 0.00000000008151\n",
      "Epoch: 417 \tTraining Loss: 0.00000000143210 \tValidation Loss: 0.00000000008034\n",
      "Epoch: 418 \tTraining Loss: 0.00000000140628 \tValidation Loss: 0.00000000007764\n",
      "Epoch: 419 \tTraining Loss: 0.00000000137955 \tValidation Loss: 0.00000000007695\n",
      "Epoch: 420 \tTraining Loss: 0.00000000135148 \tValidation Loss: 0.00000000007549\n",
      "Epoch: 421 \tTraining Loss: 0.00000000133025 \tValidation Loss: 0.00000000007400\n",
      "Epoch: 422 \tTraining Loss: 0.00000000130786 \tValidation Loss: 0.00000000007303\n",
      "Validation loss decreased (0.00000000007322 --> 0.00000000007303).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 423 \tTraining Loss: 0.00000000128398 \tValidation Loss: 0.00000000007190\n",
      "Validation loss decreased (0.00000000007303 --> 0.00000000007190).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 424 \tTraining Loss: 0.00000000126027 \tValidation Loss: 0.00000000007123\n",
      "Validation loss decreased (0.00000000007190 --> 0.00000000007123).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 425 \tTraining Loss: 0.00000000124212 \tValidation Loss: 0.00000000006903\n",
      "Validation loss decreased (0.00000000007123 --> 0.00000000006903).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 426 \tTraining Loss: 0.00000000121983 \tValidation Loss: 0.00000000006914\n",
      "Epoch: 427 \tTraining Loss: 0.00000000120111 \tValidation Loss: 0.00000000006776\n",
      "Validation loss decreased (0.00000000006903 --> 0.00000000006776).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 428 \tTraining Loss: 0.00000000117792 \tValidation Loss: 0.00000000006760\n",
      "Validation loss decreased (0.00000000006776 --> 0.00000000006760).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 429 \tTraining Loss: 0.00000000116384 \tValidation Loss: 0.00000000006519\n",
      "Validation loss decreased (0.00000000006760 --> 0.00000000006519).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 430 \tTraining Loss: 0.00000000114212 \tValidation Loss: 0.00000000006376\n",
      "Validation loss decreased (0.00000000006519 --> 0.00000000006376).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 431 \tTraining Loss: 0.00000000112291 \tValidation Loss: 0.00000000006359\n",
      "Validation loss decreased (0.00000000006376 --> 0.00000000006359).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 432 \tTraining Loss: 0.00000000110569 \tValidation Loss: 0.00000000006273\n",
      "Validation loss decreased (0.00000000006359 --> 0.00000000006273).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 433 \tTraining Loss: 0.00000000108732 \tValidation Loss: 0.00000000006098\n",
      "Validation loss decreased (0.00000000006273 --> 0.00000000006098).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 434 \tTraining Loss: 0.00000000106873 \tValidation Loss: 0.00000000006055\n",
      "Validation loss decreased (0.00000000006098 --> 0.00000000006055).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 435 \tTraining Loss: 0.00000000104971 \tValidation Loss: 0.00000000005851\n",
      "Validation loss decreased (0.00000000006055 --> 0.00000000005851).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 436 \tTraining Loss: 0.00000000103010 \tValidation Loss: 0.00000000005841\n",
      "Validation loss decreased (0.00000000005851 --> 0.00000000005841).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 437 \tTraining Loss: 0.00000000101410 \tValidation Loss: 0.00000000005674\n",
      "Validation loss decreased (0.00000000005841 --> 0.00000000005674).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 438 \tTraining Loss: 0.00000000098949 \tValidation Loss: 0.00000000005646\n",
      "Validation loss decreased (0.00000000005674 --> 0.00000000005646).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 439 \tTraining Loss: 0.00000000097593 \tValidation Loss: 0.00000000005573\n",
      "Validation loss decreased (0.00000000005646 --> 0.00000000005573).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 440 \tTraining Loss: 0.00000000096142 \tValidation Loss: 0.00000000005377\n",
      "Validation loss decreased (0.00000000005573 --> 0.00000000005377).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 441 \tTraining Loss: 0.00000000094133 \tValidation Loss: 0.00000000005368\n",
      "Validation loss decreased (0.00000000005377 --> 0.00000000005368).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 442 \tTraining Loss: 0.00000000092588 \tValidation Loss: 0.00000000005148\n",
      "Validation loss decreased (0.00000000005368 --> 0.00000000005148).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 443 \tTraining Loss: 0.00000000090698 \tValidation Loss: 0.00000000005079\n",
      "Validation loss decreased (0.00000000005148 --> 0.00000000005079).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 444 \tTraining Loss: 0.00000000089020 \tValidation Loss: 0.00000000005007\n",
      "Validation loss decreased (0.00000000005079 --> 0.00000000005007).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 445 \tTraining Loss: 0.00000000087449 \tValidation Loss: 0.00000000005024\n",
      "Epoch: 446 \tTraining Loss: 0.00000000085732 \tValidation Loss: 0.00000000004994\n",
      "Validation loss decreased (0.00000000005007 --> 0.00000000004994).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 447 \tTraining Loss: 0.00000000084706 \tValidation Loss: 0.00000000004712\n",
      "Validation loss decreased (0.00000000004994 --> 0.00000000004712).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 448 \tTraining Loss: 0.00000000082467 \tValidation Loss: 0.00000000004755\n",
      "Epoch: 449 \tTraining Loss: 0.00000000081182 \tValidation Loss: 0.00000000004569\n",
      "Validation loss decreased (0.00000000004712 --> 0.00000000004569).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 450 \tTraining Loss: 0.00000000079764 \tValidation Loss: 0.00000000004683\n",
      "Epoch: 451 \tTraining Loss: 0.00000000078594 \tValidation Loss: 0.00000000004409\n",
      "Validation loss decreased (0.00000000004569 --> 0.00000000004409).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 452 \tTraining Loss: 0.00000000077024 \tValidation Loss: 0.00000000004349\n",
      "Validation loss decreased (0.00000000004409 --> 0.00000000004349).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 453 \tTraining Loss: 0.00000000076141 \tValidation Loss: 0.00000000004292\n",
      "Validation loss decreased (0.00000000004349 --> 0.00000000004292).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 454 \tTraining Loss: 0.00000000074955 \tValidation Loss: 0.00000000004485\n",
      "Epoch: 455 \tTraining Loss: 0.00000000073990 \tValidation Loss: 0.00000000004245\n",
      "Validation loss decreased (0.00000000004292 --> 0.00000000004245).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 456 \tTraining Loss: 0.00000000073237 \tValidation Loss: 0.00000000004133\n",
      "Validation loss decreased (0.00000000004245 --> 0.00000000004133).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 457 \tTraining Loss: 0.00000000072585 \tValidation Loss: 0.00000000004152\n",
      "Epoch: 458 \tTraining Loss: 0.00000000071483 \tValidation Loss: 0.00000000004077\n",
      "Validation loss decreased (0.00000000004133 --> 0.00000000004077).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 459 \tTraining Loss: 0.00000000070741 \tValidation Loss: 0.00000000003949\n",
      "Validation loss decreased (0.00000000004077 --> 0.00000000003949).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 460 \tTraining Loss: 0.00000000069918 \tValidation Loss: 0.00000000003971\n",
      "Epoch: 461 \tTraining Loss: 0.00000000069469 \tValidation Loss: 0.00000000003859\n",
      "Validation loss decreased (0.00000000003949 --> 0.00000000003859).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 462 \tTraining Loss: 0.00000000068529 \tValidation Loss: 0.00000000003843\n",
      "Validation loss decreased (0.00000000003859 --> 0.00000000003843).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 463 \tTraining Loss: 0.00000000068080 \tValidation Loss: 0.00000000003779\n",
      "Validation loss decreased (0.00000000003843 --> 0.00000000003779).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 464 \tTraining Loss: 0.00000000067247 \tValidation Loss: 0.00000000003884\n",
      "Epoch: 465 \tTraining Loss: 0.00000000066983 \tValidation Loss: 0.00000000003845\n",
      "Epoch: 466 \tTraining Loss: 0.00000000066206 \tValidation Loss: 0.00000000003817\n",
      "Epoch: 467 \tTraining Loss: 0.00000000065791 \tValidation Loss: 0.00000000003763\n",
      "Validation loss decreased (0.00000000003779 --> 0.00000000003763).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 468 \tTraining Loss: 0.00000000065302 \tValidation Loss: 0.00000000003744\n",
      "Validation loss decreased (0.00000000003763 --> 0.00000000003744).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 469 \tTraining Loss: 0.00000000064973 \tValidation Loss: 0.00000000003697\n",
      "Validation loss decreased (0.00000000003744 --> 0.00000000003697).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 470 \tTraining Loss: 0.00000000064341 \tValidation Loss: 0.00000000003656\n",
      "Validation loss decreased (0.00000000003697 --> 0.00000000003656).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 471 \tTraining Loss: 0.00000000063779 \tValidation Loss: 0.00000000003632\n",
      "Validation loss decreased (0.00000000003656 --> 0.00000000003632).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 472 \tTraining Loss: 0.00000000063606 \tValidation Loss: 0.00000000003498\n",
      "Validation loss decreased (0.00000000003632 --> 0.00000000003498).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 473 \tTraining Loss: 0.00000000063349 \tValidation Loss: 0.00000000003552\n",
      "Epoch: 474 \tTraining Loss: 0.00000000062248 \tValidation Loss: 0.00000000003709\n",
      "Epoch: 475 \tTraining Loss: 0.00000000062646 \tValidation Loss: 0.00000000003505\n",
      "Epoch: 476 \tTraining Loss: 0.00000000061970 \tValidation Loss: 0.00000000003451\n",
      "Validation loss decreased (0.00000000003498 --> 0.00000000003451).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 477 \tTraining Loss: 0.00000000061365 \tValidation Loss: 0.00000000003545\n",
      "Epoch: 478 \tTraining Loss: 0.00000000061108 \tValidation Loss: 0.00000000003491\n",
      "Epoch: 479 \tTraining Loss: 0.00000000060949 \tValidation Loss: 0.00000000003373\n",
      "Validation loss decreased (0.00000000003451 --> 0.00000000003373).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 480 \tTraining Loss: 0.00000000060296 \tValidation Loss: 0.00000000003388\n",
      "Epoch: 481 \tTraining Loss: 0.00000000060141 \tValidation Loss: 0.00000000003340\n",
      "Validation loss decreased (0.00000000003373 --> 0.00000000003340).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 482 \tTraining Loss: 0.00000000059657 \tValidation Loss: 0.00000000003327\n",
      "Validation loss decreased (0.00000000003340 --> 0.00000000003327).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 483 \tTraining Loss: 0.00000000059633 \tValidation Loss: 0.00000000003319\n",
      "Validation loss decreased (0.00000000003327 --> 0.00000000003319).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 484 \tTraining Loss: 0.00000000059240 \tValidation Loss: 0.00000000003284\n",
      "Validation loss decreased (0.00000000003319 --> 0.00000000003284).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 485 \tTraining Loss: 0.00000000058690 \tValidation Loss: 0.00000000003261\n",
      "Validation loss decreased (0.00000000003284 --> 0.00000000003261).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 486 \tTraining Loss: 0.00000000058398 \tValidation Loss: 0.00000000003280\n",
      "Epoch: 487 \tTraining Loss: 0.00000000058131 \tValidation Loss: 0.00000000003312\n",
      "Epoch: 488 \tTraining Loss: 0.00000000058122 \tValidation Loss: 0.00000000003224\n",
      "Validation loss decreased (0.00000000003261 --> 0.00000000003224).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 489 \tTraining Loss: 0.00000000057610 \tValidation Loss: 0.00000000003267\n",
      "Epoch: 490 \tTraining Loss: 0.00000000057375 \tValidation Loss: 0.00000000003329\n",
      "Epoch: 491 \tTraining Loss: 0.00000000057183 \tValidation Loss: 0.00000000003278\n",
      "Epoch: 492 \tTraining Loss: 0.00000000056887 \tValidation Loss: 0.00000000003276\n",
      "Epoch: 493 \tTraining Loss: 0.00000000056623 \tValidation Loss: 0.00000000003172\n",
      "Validation loss decreased (0.00000000003224 --> 0.00000000003172).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 494 \tTraining Loss: 0.00000000056248 \tValidation Loss: 0.00000000003276\n",
      "Epoch: 495 \tTraining Loss: 0.00000000056390 \tValidation Loss: 0.00000000003107\n",
      "Validation loss decreased (0.00000000003172 --> 0.00000000003107).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 496 \tTraining Loss: 0.00000000055699 \tValidation Loss: 0.00000000003198\n",
      "Epoch: 497 \tTraining Loss: 0.00000000055552 \tValidation Loss: 0.00000000003139\n",
      "Epoch: 498 \tTraining Loss: 0.00000000055088 \tValidation Loss: 0.00000000003140\n",
      "Epoch: 499 \tTraining Loss: 0.00000000055354 \tValidation Loss: 0.00000000003107\n",
      "Validation loss decreased (0.00000000003107 --> 0.00000000003107).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 500 \tTraining Loss: 0.00000000054879 \tValidation Loss: 0.00000000003329\n",
      "Epoch: 501 \tTraining Loss: 0.00000000054646 \tValidation Loss: 0.00000000003278\n",
      "results/results_n=1.0\n",
      "90000\n",
      "Epoch: 502 \tTraining Loss: 0.00000000054764 \tValidation Loss: 0.00000000003075\n",
      "Validation loss decreased (0.00000000003107 --> 0.00000000003075).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 503 \tTraining Loss: 0.00000000053925 \tValidation Loss: 0.00000000003267\n",
      "Epoch: 504 \tTraining Loss: 0.00000000054149 \tValidation Loss: 0.00000000003027\n",
      "Validation loss decreased (0.00000000003075 --> 0.00000000003027).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 505 \tTraining Loss: 0.00000000053788 \tValidation Loss: 0.00000000003064\n",
      "Epoch: 506 \tTraining Loss: 0.00000000053579 \tValidation Loss: 0.00000000002891\n",
      "Validation loss decreased (0.00000000003027 --> 0.00000000002891).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 507 \tTraining Loss: 0.00000000053193 \tValidation Loss: 0.00000000003165\n",
      "Epoch: 508 \tTraining Loss: 0.00000000053160 \tValidation Loss: 0.00000000003062\n",
      "Epoch: 509 \tTraining Loss: 0.00000000052992 \tValidation Loss: 0.00000000003053\n",
      "Epoch: 510 \tTraining Loss: 0.00000000052838 \tValidation Loss: 0.00000000003017\n",
      "Epoch: 511 \tTraining Loss: 0.00000000052234 \tValidation Loss: 0.00000000003004\n",
      "Epoch: 512 \tTraining Loss: 0.00000000052480 \tValidation Loss: 0.00000000003103\n",
      "Epoch: 513 \tTraining Loss: 0.00000000052318 \tValidation Loss: 0.00000000002954\n",
      "Epoch: 514 \tTraining Loss: 0.00000000051897 \tValidation Loss: 0.00000000002909\n",
      "Epoch: 515 \tTraining Loss: 0.00000000051649 \tValidation Loss: 0.00000000003038\n",
      "Epoch: 516 \tTraining Loss: 0.00000000051634 \tValidation Loss: 0.00000000002837\n",
      "Validation loss decreased (0.00000000002891 --> 0.00000000002837).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 517 \tTraining Loss: 0.00000000051355 \tValidation Loss: 0.00000000003019\n",
      "Epoch: 518 \tTraining Loss: 0.00000000051500 \tValidation Loss: 0.00000000002827\n",
      "Validation loss decreased (0.00000000002837 --> 0.00000000002827).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 519 \tTraining Loss: 0.00000000051167 \tValidation Loss: 0.00000000002814\n",
      "Validation loss decreased (0.00000000002827 --> 0.00000000002814).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 520 \tTraining Loss: 0.00000000050733 \tValidation Loss: 0.00000000002870\n",
      "Epoch: 521 \tTraining Loss: 0.00000000050565 \tValidation Loss: 0.00000000003075\n",
      "Epoch: 522 \tTraining Loss: 0.00000000050448 \tValidation Loss: 0.00000000002848\n",
      "Epoch: 523 \tTraining Loss: 0.00000000051115 \tValidation Loss: 0.00000000002773\n",
      "Validation loss decreased (0.00000000002814 --> 0.00000000002773).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 524 \tTraining Loss: 0.00000000050012 \tValidation Loss: 0.00000000002915\n",
      "Epoch: 525 \tTraining Loss: 0.00000000050014 \tValidation Loss: 0.00000000002882\n",
      "Epoch: 526 \tTraining Loss: 0.00000000049772 \tValidation Loss: 0.00000000002917\n",
      "Epoch: 527 \tTraining Loss: 0.00000000049688 \tValidation Loss: 0.00000000002740\n",
      "Validation loss decreased (0.00000000002773 --> 0.00000000002740).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 528 \tTraining Loss: 0.00000000049692 \tValidation Loss: 0.00000000002801\n",
      "Epoch: 529 \tTraining Loss: 0.00000000049144 \tValidation Loss: 0.00000000002941\n",
      "Epoch: 530 \tTraining Loss: 0.00000000049068 \tValidation Loss: 0.00000000002798\n",
      "Epoch: 531 \tTraining Loss: 0.00000000049133 \tValidation Loss: 0.00000000002900\n",
      "Epoch: 532 \tTraining Loss: 0.00000000048621 \tValidation Loss: 0.00000000002865\n",
      "Epoch: 533 \tTraining Loss: 0.00000000048412 \tValidation Loss: 0.00000000002777\n",
      "Epoch: 534 \tTraining Loss: 0.00000000048717 \tValidation Loss: 0.00000000002781\n",
      "Epoch: 535 \tTraining Loss: 0.00000000048386 \tValidation Loss: 0.00000000002861\n",
      "Epoch: 536 \tTraining Loss: 0.00000000048328 \tValidation Loss: 0.00000000002941\n",
      "Epoch: 537 \tTraining Loss: 0.00000000048097 \tValidation Loss: 0.00000000002742\n",
      "Epoch: 538 \tTraining Loss: 0.00000000047907 \tValidation Loss: 0.00000000002801\n",
      "Epoch: 539 \tTraining Loss: 0.00000000047607 \tValidation Loss: 0.00000000002716\n",
      "Validation loss decreased (0.00000000002740 --> 0.00000000002716).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 540 \tTraining Loss: 0.00000000047307 \tValidation Loss: 0.00000000002777\n",
      "Epoch: 541 \tTraining Loss: 0.00000000047414 \tValidation Loss: 0.00000000002727\n",
      "Epoch: 542 \tTraining Loss: 0.00080561867069 \tValidation Loss: 0.00000003881963\n",
      "Epoch: 543 \tTraining Loss: 0.00000043757819 \tValidation Loss: 0.00000002482786\n",
      "Epoch: 544 \tTraining Loss: 0.00000031529058 \tValidation Loss: 0.00000001875940\n",
      "Epoch: 545 \tTraining Loss: 0.00000025210675 \tValidation Loss: 0.00000001515072\n",
      "Epoch: 546 \tTraining Loss: 0.00000021131571 \tValidation Loss: 0.00000001274131\n",
      "Epoch: 547 \tTraining Loss: 0.00000018212356 \tValidation Loss: 0.00000001099994\n",
      "Epoch: 548 \tTraining Loss: 0.00000015978312 \tValidation Loss: 0.00000000965931\n",
      "Epoch: 549 \tTraining Loss: 0.00000014196791 \tValidation Loss: 0.00000000858572\n",
      "Epoch: 550 \tTraining Loss: 0.00000012731430 \tValidation Loss: 0.00000000769939\n",
      "Epoch: 551 \tTraining Loss: 0.00000011494005 \tValidation Loss: 0.00000000695651\n",
      "Epoch: 552 \tTraining Loss: 0.00000010441287 \tValidation Loss: 0.00000000632366\n",
      "Epoch: 553 \tTraining Loss: 0.00000009526236 \tValidation Loss: 0.00000000577136\n",
      "Epoch: 554 \tTraining Loss: 0.00000008724973 \tValidation Loss: 0.00000000529290\n",
      "Epoch: 555 \tTraining Loss: 0.00000008017800 \tValidation Loss: 0.00000000486667\n",
      "Epoch: 556 \tTraining Loss: 0.00000007388004 \tValidation Loss: 0.00000000449403\n",
      "Epoch: 557 \tTraining Loss: 0.00000006825100 \tValidation Loss: 0.00000000415384\n",
      "Epoch: 558 \tTraining Loss: 0.00000006318067 \tValidation Loss: 0.00000000385353\n",
      "Epoch: 559 \tTraining Loss: 0.00000005861864 \tValidation Loss: 0.00000000358000\n",
      "Epoch: 560 \tTraining Loss: 0.00000005446699 \tValidation Loss: 0.00000000332934\n",
      "Epoch: 561 \tTraining Loss: 0.00000005070304 \tValidation Loss: 0.00000000310499\n",
      "Epoch: 562 \tTraining Loss: 0.00000004726482 \tValidation Loss: 0.00000000289617\n",
      "Epoch: 563 \tTraining Loss: 0.00000004412613 \tValidation Loss: 0.00000000270625\n",
      "Epoch: 564 \tTraining Loss: 0.00000004124138 \tValidation Loss: 0.00000000253208\n",
      "Epoch: 565 \tTraining Loss: 0.00000003858739 \tValidation Loss: 0.00000000237345\n",
      "Epoch: 566 \tTraining Loss: 0.00000003615377 \tValidation Loss: 0.00000000222314\n",
      "Epoch: 567 \tTraining Loss: 0.00000003390051 \tValidation Loss: 0.00000000208716\n",
      "Epoch: 568 \tTraining Loss: 0.00000003181810 \tValidation Loss: 0.00000000195930\n",
      "Epoch: 569 \tTraining Loss: 0.00000002989415 \tValidation Loss: 0.00000000184213\n",
      "Epoch: 570 \tTraining Loss: 0.00000002811092 \tValidation Loss: 0.00000000173102\n",
      "Epoch: 571 \tTraining Loss: 0.00000002645686 \tValidation Loss: 0.00000000162855\n",
      "Epoch: 572 \tTraining Loss: 0.00000002492569 \tValidation Loss: 0.00000000153395\n",
      "Epoch: 573 \tTraining Loss: 0.00000002349378 \tValidation Loss: 0.00000000144513\n",
      "Epoch: 574 \tTraining Loss: 0.00000002216498 \tValidation Loss: 0.00000000136325\n",
      "Epoch: 575 \tTraining Loss: 0.00000002093086 \tValidation Loss: 0.00000000128686\n",
      "Epoch: 576 \tTraining Loss: 0.00000001977179 \tValidation Loss: 0.00000000121418\n",
      "Epoch: 577 \tTraining Loss: 0.00000001869868 \tValidation Loss: 0.00000000114654\n",
      "Epoch: 578 \tTraining Loss: 0.00000001769332 \tValidation Loss: 0.00000000108498\n",
      "Epoch: 579 \tTraining Loss: 0.00000001675459 \tValidation Loss: 0.00000000102554\n",
      "Epoch: 580 \tTraining Loss: 0.00000001587547 \tValidation Loss: 0.00000000097130\n",
      "Epoch: 581 \tTraining Loss: 0.00000001504200 \tValidation Loss: 0.00000000092062\n",
      "Epoch: 582 \tTraining Loss: 0.00000001426987 \tValidation Loss: 0.00000000087310\n",
      "Epoch: 583 \tTraining Loss: 0.00000001354248 \tValidation Loss: 0.00000000082799\n",
      "Epoch: 584 \tTraining Loss: 0.00000001286287 \tValidation Loss: 0.00000000078537\n",
      "Epoch: 585 \tTraining Loss: 0.00000001222376 \tValidation Loss: 0.00000000074409\n",
      "Epoch: 586 \tTraining Loss: 0.00000001161832 \tValidation Loss: 0.00000000070704\n",
      "Epoch: 587 \tTraining Loss: 0.00000001105002 \tValidation Loss: 0.00000000067104\n",
      "Epoch: 588 \tTraining Loss: 0.00000001051138 \tValidation Loss: 0.00000000063839\n",
      "Epoch: 589 \tTraining Loss: 0.00000001000779 \tValidation Loss: 0.00000000060538\n",
      "Epoch: 590 \tTraining Loss: 0.00000000952588 \tValidation Loss: 0.00000000057710\n",
      "Epoch: 591 \tTraining Loss: 0.00000000907518 \tValidation Loss: 0.00000000054905\n",
      "Epoch: 592 \tTraining Loss: 0.00000000864775 \tValidation Loss: 0.00000000052180\n",
      "Epoch: 593 \tTraining Loss: 0.00000000823719 \tValidation Loss: 0.00000000049636\n",
      "Epoch: 594 \tTraining Loss: 0.00000000785112 \tValidation Loss: 0.00000000047255\n",
      "Epoch: 595 \tTraining Loss: 0.00000000748581 \tValidation Loss: 0.00000000045007\n",
      "Epoch: 596 \tTraining Loss: 0.00000000713910 \tValidation Loss: 0.00000000042886\n",
      "Epoch: 597 \tTraining Loss: 0.00000000680747 \tValidation Loss: 0.00000000040867\n",
      "Epoch: 598 \tTraining Loss: 0.00000000649116 \tValidation Loss: 0.00000000038987\n",
      "Epoch: 599 \tTraining Loss: 0.00000000618788 \tValidation Loss: 0.00000000037121\n",
      "Epoch: 600 \tTraining Loss: 0.00000000590159 \tValidation Loss: 0.00000000035366\n",
      "Epoch: 601 \tTraining Loss: 0.00000000562700 \tValidation Loss: 0.00000000033718\n",
      "results/results_n=1.0\n",
      "90000\n",
      "Epoch: 602 \tTraining Loss: 0.00000000536813 \tValidation Loss: 0.00000000032190\n",
      "Epoch: 603 \tTraining Loss: 0.00000000512312 \tValidation Loss: 0.00000000030689\n",
      "Epoch: 604 \tTraining Loss: 0.00000000488520 \tValidation Loss: 0.00000000029257\n",
      "Epoch: 605 \tTraining Loss: 0.00000000466116 \tValidation Loss: 0.00000000027876\n",
      "Epoch: 606 \tTraining Loss: 0.00000000444839 \tValidation Loss: 0.00000000026660\n",
      "Epoch: 607 \tTraining Loss: 0.00000000424413 \tValidation Loss: 0.00000000025405\n",
      "Epoch: 608 \tTraining Loss: 0.00000000405003 \tValidation Loss: 0.00000000024201\n",
      "Epoch: 609 \tTraining Loss: 0.00000000386995 \tValidation Loss: 0.00000000023088\n",
      "Epoch: 610 \tTraining Loss: 0.00000000369307 \tValidation Loss: 0.00000000022037\n",
      "Epoch: 611 \tTraining Loss: 0.00000000352709 \tValidation Loss: 0.00000000021061\n",
      "Epoch: 612 \tTraining Loss: 0.00000000337266 \tValidation Loss: 0.00000000020199\n",
      "Epoch: 613 \tTraining Loss: 0.00000000322486 \tValidation Loss: 0.00000000019325\n",
      "Epoch: 614 \tTraining Loss: 0.00000000308145 \tValidation Loss: 0.00000000018455\n",
      "Epoch: 615 \tTraining Loss: 0.00000000294505 \tValidation Loss: 0.00000000017714\n",
      "Epoch: 616 \tTraining Loss: 0.00000000281582 \tValidation Loss: 0.00000000016894\n",
      "Epoch: 617 \tTraining Loss: 0.00000000269545 \tValidation Loss: 0.00000000016166\n",
      "Epoch: 618 \tTraining Loss: 0.00000000258267 \tValidation Loss: 0.00000000015477\n",
      "Epoch: 619 \tTraining Loss: 0.00000000247378 \tValidation Loss: 0.00000000014754\n",
      "Epoch: 620 \tTraining Loss: 0.00000000237068 \tValidation Loss: 0.00000000014156\n",
      "Epoch: 621 \tTraining Loss: 0.00000000227295 \tValidation Loss: 0.00000000013525\n",
      "Epoch: 622 \tTraining Loss: 0.00000000218017 \tValidation Loss: 0.00000000012979\n",
      "Epoch: 623 \tTraining Loss: 0.00000000209326 \tValidation Loss: 0.00000000012433\n",
      "Epoch: 624 \tTraining Loss: 0.00000000201104 \tValidation Loss: 0.00000000011994\n",
      "Epoch: 625 \tTraining Loss: 0.00000000193220 \tValidation Loss: 0.00000000011535\n",
      "Epoch: 626 \tTraining Loss: 0.00000000185948 \tValidation Loss: 0.00000000011027\n",
      "Epoch: 627 \tTraining Loss: 0.00000000178181 \tValidation Loss: 0.00000000010615\n",
      "Epoch: 628 \tTraining Loss: 0.00000000171473 \tValidation Loss: 0.00000000010200\n",
      "Epoch: 629 \tTraining Loss: 0.00000000165405 \tValidation Loss: 0.00000000009807\n",
      "Epoch: 630 \tTraining Loss: 0.00000000158962 \tValidation Loss: 0.00000000009436\n",
      "Epoch: 631 \tTraining Loss: 0.00000000152942 \tValidation Loss: 0.00000000009106\n",
      "Epoch: 632 \tTraining Loss: 0.00000000147542 \tValidation Loss: 0.00000000008781\n",
      "Epoch: 633 \tTraining Loss: 0.00000000142163 \tValidation Loss: 0.00000000008458\n",
      "Epoch: 634 \tTraining Loss: 0.00000000137074 \tValidation Loss: 0.00000000008140\n",
      "Epoch: 635 \tTraining Loss: 0.00000000132369 \tValidation Loss: 0.00000000007827\n",
      "Epoch: 636 \tTraining Loss: 0.00000000127912 \tValidation Loss: 0.00000000007626\n",
      "Epoch: 637 \tTraining Loss: 0.00000000123747 \tValidation Loss: 0.00000000007385\n",
      "Epoch: 638 \tTraining Loss: 0.00000000119720 \tValidation Loss: 0.00000000007223\n",
      "Epoch: 639 \tTraining Loss: 0.00000000116032 \tValidation Loss: 0.00000000007031\n",
      "Epoch: 640 \tTraining Loss: 0.00000000112595 \tValidation Loss: 0.00000000006806\n",
      "Epoch: 641 \tTraining Loss: 0.00000000109011 \tValidation Loss: 0.00000000006618\n",
      "Epoch: 642 \tTraining Loss: 0.00000000106301 \tValidation Loss: 0.00000000006367\n",
      "Epoch: 643 \tTraining Loss: 0.00000000102837 \tValidation Loss: 0.00000000006124\n",
      "Epoch: 644 \tTraining Loss: 0.00000000099747 \tValidation Loss: 0.00000000006013\n",
      "Epoch: 645 \tTraining Loss: 0.00000000097167 \tValidation Loss: 0.00000000005787\n",
      "Epoch: 646 \tTraining Loss: 0.00000000094488 \tValidation Loss: 0.00000000005662\n",
      "Epoch: 647 \tTraining Loss: 0.00000000314979 \tValidation Loss: 0.00000000010440\n",
      "Epoch: 648 \tTraining Loss: 0.00000000150574 \tValidation Loss: 0.00000000008125\n",
      "Epoch: 649 \tTraining Loss: 0.00000000128282 \tValidation Loss: 0.00000000007337\n",
      "Epoch: 650 \tTraining Loss: 0.00000000117848 \tValidation Loss: 0.00000000006882\n",
      "Epoch: 651 \tTraining Loss: 0.00000000110604 \tValidation Loss: 0.00000000006413\n",
      "Epoch: 652 \tTraining Loss: 0.00000000105195 \tValidation Loss: 0.00000000006093\n",
      "Epoch: 653 \tTraining Loss: 0.00000000100207 \tValidation Loss: 0.00000000005938\n",
      "Epoch: 654 \tTraining Loss: 0.00000000096679 \tValidation Loss: 0.00000000005700\n",
      "Epoch: 655 \tTraining Loss: 0.00000000093397 \tValidation Loss: 0.00000000005400\n",
      "Epoch: 656 \tTraining Loss: 0.00000000089906 \tValidation Loss: 0.00000000005309\n",
      "Epoch: 657 \tTraining Loss: 0.00000000087090 \tValidation Loss: 0.00000000005165\n",
      "Epoch: 658 \tTraining Loss: 0.00000000084972 \tValidation Loss: 0.00000000004968\n",
      "Epoch: 659 \tTraining Loss: 0.00000000082226 \tValidation Loss: 0.00000000004882\n",
      "Epoch: 660 \tTraining Loss: 0.00000000080287 \tValidation Loss: 0.00000000004729\n",
      "Epoch: 661 \tTraining Loss: 0.00000000078110 \tValidation Loss: 0.00000000004664\n",
      "Epoch: 662 \tTraining Loss: 0.00000000076298 \tValidation Loss: 0.00000000004562\n",
      "Epoch: 663 \tTraining Loss: 0.00000000074839 \tValidation Loss: 0.00000000004441\n",
      "Epoch: 664 \tTraining Loss: 0.00000000072498 \tValidation Loss: 0.00000000004413\n",
      "Epoch: 665 \tTraining Loss: 0.00000000071317 \tValidation Loss: 0.00000000004286\n",
      "Epoch: 666 \tTraining Loss: 0.00000000069503 \tValidation Loss: 0.00000000004163\n",
      "Epoch: 667 \tTraining Loss: 0.00000000068024 \tValidation Loss: 0.00000000004070\n",
      "Epoch: 668 \tTraining Loss: 0.00000000066506 \tValidation Loss: 0.00000000003885\n",
      "Epoch: 669 \tTraining Loss: 0.00000000064822 \tValidation Loss: 0.00000000003759\n",
      "Epoch: 670 \tTraining Loss: 0.00000000063190 \tValidation Loss: 0.00000000003764\n",
      "Epoch: 671 \tTraining Loss: 0.00000000061631 \tValidation Loss: 0.00000000003593\n",
      "Epoch: 672 \tTraining Loss: 0.00000000060020 \tValidation Loss: 0.00000000003617\n",
      "Epoch: 673 \tTraining Loss: 0.00000000058761 \tValidation Loss: 0.00000000003429\n",
      "Epoch: 674 \tTraining Loss: 0.00000000057185 \tValidation Loss: 0.00000000003323\n",
      "Epoch: 675 \tTraining Loss: 0.00000000055384 \tValidation Loss: 0.00000000003323\n",
      "Epoch: 676 \tTraining Loss: 0.00000000054603 \tValidation Loss: 0.00000000003178\n",
      "Epoch: 677 \tTraining Loss: 0.00000000053119 \tValidation Loss: 0.00000000003207\n",
      "Epoch: 678 \tTraining Loss: 0.00000000052286 \tValidation Loss: 0.00000000003098\n",
      "Epoch: 679 \tTraining Loss: 0.00000000051383 \tValidation Loss: 0.00000000003003\n",
      "Epoch: 680 \tTraining Loss: 0.00000000050517 \tValidation Loss: 0.00000000003019\n",
      "Epoch: 681 \tTraining Loss: 0.00000000050062 \tValidation Loss: 0.00000000002928\n",
      "Epoch: 682 \tTraining Loss: 0.00000000049125 \tValidation Loss: 0.00000000002902\n",
      "Epoch: 683 \tTraining Loss: 0.00000000048434 \tValidation Loss: 0.00000000002926\n",
      "Epoch: 684 \tTraining Loss: 0.00000000047894 \tValidation Loss: 0.00000000002934\n",
      "Epoch: 685 \tTraining Loss: 0.00000000047611 \tValidation Loss: 0.00000000002850\n",
      "Epoch: 686 \tTraining Loss: 0.00000000046834 \tValidation Loss: 0.00000000002904\n",
      "Epoch: 687 \tTraining Loss: 0.00000000046572 \tValidation Loss: 0.00000000002772\n",
      "Epoch: 688 \tTraining Loss: 0.00000000046004 \tValidation Loss: 0.00000000002816\n",
      "Epoch: 689 \tTraining Loss: 0.00000000045879 \tValidation Loss: 0.00000000002772\n",
      "Epoch: 690 \tTraining Loss: 0.00000000045255 \tValidation Loss: 0.00000000002766\n",
      "Epoch: 691 \tTraining Loss: 0.00000000044573 \tValidation Loss: 0.00000000002766\n",
      "Epoch: 692 \tTraining Loss: 0.00000000044663 \tValidation Loss: 0.00000000002775\n",
      "Epoch: 693 \tTraining Loss: 0.00000000044275 \tValidation Loss: 0.00000000002658\n",
      "Validation loss decreased (0.00000000002716 --> 0.00000000002658).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 694 \tTraining Loss: 0.00000000043673 \tValidation Loss: 0.00000000002678\n",
      "Epoch: 695 \tTraining Loss: 0.00000000043452 \tValidation Loss: 0.00000000002665\n",
      "Epoch: 696 \tTraining Loss: 0.00000000043081 \tValidation Loss: 0.00000000002567\n",
      "Validation loss decreased (0.00000000002658 --> 0.00000000002567).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 697 \tTraining Loss: 0.00000000042601 \tValidation Loss: 0.00000000002628\n",
      "Epoch: 698 \tTraining Loss: 0.00000000042433 \tValidation Loss: 0.00000000002630\n",
      "Epoch: 699 \tTraining Loss: 0.00000000041697 \tValidation Loss: 0.00000000002643\n",
      "Epoch: 700 \tTraining Loss: 0.00000000041634 \tValidation Loss: 0.00000000002563\n",
      "Validation loss decreased (0.00000000002567 --> 0.00000000002563).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 701 \tTraining Loss: 0.00000000041209 \tValidation Loss: 0.00000000002602\n",
      "results/results_n=1.0\n",
      "90000\n",
      "Epoch: 702 \tTraining Loss: 0.00000000041036 \tValidation Loss: 0.00000000002598\n",
      "Epoch: 703 \tTraining Loss: 0.00000000040835 \tValidation Loss: 0.00000000002414\n",
      "Validation loss decreased (0.00000000002563 --> 0.00000000002414).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 704 \tTraining Loss: 0.00000000040222 \tValidation Loss: 0.00000000002446\n",
      "Epoch: 705 \tTraining Loss: 0.00000000039963 \tValidation Loss: 0.00000000002490\n",
      "Epoch: 706 \tTraining Loss: 0.00000000039723 \tValidation Loss: 0.00000000002464\n",
      "Epoch: 707 \tTraining Loss: 0.00000000039535 \tValidation Loss: 0.00000000002423\n",
      "Epoch: 708 \tTraining Loss: 0.00000000039166 \tValidation Loss: 0.00000000002401\n",
      "Validation loss decreased (0.00000000002414 --> 0.00000000002401).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 709 \tTraining Loss: 0.00000000038980 \tValidation Loss: 0.00000000002336\n",
      "Validation loss decreased (0.00000000002401 --> 0.00000000002336).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 710 \tTraining Loss: 0.00000000038600 \tValidation Loss: 0.00000000002332\n",
      "Validation loss decreased (0.00000000002336 --> 0.00000000002332).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 711 \tTraining Loss: 0.00000000038540 \tValidation Loss: 0.00000000002310\n",
      "Validation loss decreased (0.00000000002332 --> 0.00000000002310).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 712 \tTraining Loss: 0.00000000038346 \tValidation Loss: 0.00000000002330\n",
      "Epoch: 713 \tTraining Loss: 0.00000000037830 \tValidation Loss: 0.00000000002356\n",
      "Epoch: 714 \tTraining Loss: 0.00000000037860 \tValidation Loss: 0.00000000002239\n",
      "Validation loss decreased (0.00000000002310 --> 0.00000000002239).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 715 \tTraining Loss: 0.00000000037557 \tValidation Loss: 0.00000000002300\n",
      "Epoch: 716 \tTraining Loss: 0.00000000037374 \tValidation Loss: 0.00000000002271\n",
      "Epoch: 717 \tTraining Loss: 0.00000000037115 \tValidation Loss: 0.00000000002328\n",
      "Epoch: 718 \tTraining Loss: 0.00000000037219 \tValidation Loss: 0.00000000002187\n",
      "Validation loss decreased (0.00000000002239 --> 0.00000000002187).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 719 \tTraining Loss: 0.00000000036560 \tValidation Loss: 0.00000000002235\n",
      "Epoch: 720 \tTraining Loss: 0.00000000036759 \tValidation Loss: 0.00000000002222\n",
      "Epoch: 721 \tTraining Loss: 0.00000000036258 \tValidation Loss: 0.00000000002233\n",
      "Epoch: 722 \tTraining Loss: 0.00000000036400 \tValidation Loss: 0.00000000002198\n",
      "Epoch: 723 \tTraining Loss: 0.00000000035908 \tValidation Loss: 0.00000000002261\n",
      "Epoch: 724 \tTraining Loss: 0.00000000035971 \tValidation Loss: 0.00000000002204\n",
      "Epoch: 725 \tTraining Loss: 0.00000000035927 \tValidation Loss: 0.00000000002135\n",
      "Validation loss decreased (0.00000000002187 --> 0.00000000002135).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 726 \tTraining Loss: 0.00000000035513 \tValidation Loss: 0.00000000002151\n",
      "Epoch: 727 \tTraining Loss: 0.00000000035629 \tValidation Loss: 0.00000000002041\n",
      "Validation loss decreased (0.00000000002135 --> 0.00000000002041).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 728 \tTraining Loss: 0.00000000035014 \tValidation Loss: 0.00000000002179\n",
      "Epoch: 729 \tTraining Loss: 0.00000000035236 \tValidation Loss: 0.00000000002133\n",
      "Epoch: 730 \tTraining Loss: 0.00000000034945 \tValidation Loss: 0.00000000002116\n",
      "Epoch: 731 \tTraining Loss: 0.00000000035141 \tValidation Loss: 0.00000000002034\n",
      "Validation loss decreased (0.00000000002041 --> 0.00000000002034).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 732 \tTraining Loss: 0.00000000034640 \tValidation Loss: 0.00000000002060\n",
      "Epoch: 733 \tTraining Loss: 0.00000000034463 \tValidation Loss: 0.00000000002073\n",
      "Epoch: 734 \tTraining Loss: 0.00000000034492 \tValidation Loss: 0.00000000002041\n",
      "Epoch: 735 \tTraining Loss: 0.00000000034185 \tValidation Loss: 0.00000000002043\n",
      "Epoch: 736 \tTraining Loss: 0.00000000034262 \tValidation Loss: 0.00000000002004\n",
      "Validation loss decreased (0.00000000002034 --> 0.00000000002004).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 737 \tTraining Loss: 0.00000000034042 \tValidation Loss: 0.00000000001995\n",
      "Validation loss decreased (0.00000000002004 --> 0.00000000001995).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 738 \tTraining Loss: 0.00000000033749 \tValidation Loss: 0.00000000002049\n",
      "Epoch: 739 \tTraining Loss: 0.00000000033837 \tValidation Loss: 0.00000000002045\n",
      "Epoch: 740 \tTraining Loss: 0.00000000033507 \tValidation Loss: 0.00000000002054\n",
      "Epoch: 741 \tTraining Loss: 0.00000000033628 \tValidation Loss: 0.00000000001993\n",
      "Validation loss decreased (0.00000000001995 --> 0.00000000001993).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 742 \tTraining Loss: 0.00000000033384 \tValidation Loss: 0.00000000002077\n",
      "Epoch: 743 \tTraining Loss: 0.00000000033462 \tValidation Loss: 0.00000000001919\n",
      "Validation loss decreased (0.00000000001993 --> 0.00000000001919).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 744 \tTraining Loss: 0.00000000033136 \tValidation Loss: 0.00000000001945\n",
      "Epoch: 745 \tTraining Loss: 0.00000000032915 \tValidation Loss: 0.00000000002030\n",
      "Epoch: 746 \tTraining Loss: 0.00000000032945 \tValidation Loss: 0.00000000002051\n",
      "Epoch: 747 \tTraining Loss: 0.00000000032721 \tValidation Loss: 0.00000000001950\n",
      "Epoch: 748 \tTraining Loss: 0.00000000032658 \tValidation Loss: 0.00000000001961\n",
      "Epoch: 749 \tTraining Loss: 0.00000000032503 \tValidation Loss: 0.00000000001971\n",
      "Epoch: 750 \tTraining Loss: 0.00000000032427 \tValidation Loss: 0.00000000002023\n",
      "Epoch: 751 \tTraining Loss: 0.00000000032308 \tValidation Loss: 0.00000000002025\n",
      "Epoch: 752 \tTraining Loss: 0.00000000032477 \tValidation Loss: 0.00000000001851\n",
      "Validation loss decreased (0.00000000001919 --> 0.00000000001851).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 753 \tTraining Loss: 0.00000000032067 \tValidation Loss: 0.00000000001963\n",
      "Epoch: 754 \tTraining Loss: 0.00000000031989 \tValidation Loss: 0.00000000001950\n",
      "Epoch: 755 \tTraining Loss: 0.00000000032030 \tValidation Loss: 0.00000000001892\n",
      "Epoch: 756 \tTraining Loss: 0.00000000031602 \tValidation Loss: 0.00000000001928\n",
      "Epoch: 757 \tTraining Loss: 0.00000000031801 \tValidation Loss: 0.00000000001898\n",
      "Epoch: 758 \tTraining Loss: 0.00000000031548 \tValidation Loss: 0.00000000001840\n",
      "Validation loss decreased (0.00000000001851 --> 0.00000000001840).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 759 \tTraining Loss: 0.00000000031464 \tValidation Loss: 0.00000000001807\n",
      "Validation loss decreased (0.00000000001840 --> 0.00000000001807).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 760 \tTraining Loss: 0.00000000031246 \tValidation Loss: 0.00000000001939\n",
      "Epoch: 761 \tTraining Loss: 0.00000000031179 \tValidation Loss: 0.00000000001870\n",
      "Epoch: 762 \tTraining Loss: 0.00000000031365 \tValidation Loss: 0.00000000001783\n",
      "Validation loss decreased (0.00000000001807 --> 0.00000000001783).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 763 \tTraining Loss: 0.00000000030860 \tValidation Loss: 0.00000000001876\n",
      "Epoch: 764 \tTraining Loss: 0.00000000031065 \tValidation Loss: 0.00000000001876\n",
      "Epoch: 765 \tTraining Loss: 0.00000000030937 \tValidation Loss: 0.00000000001809\n",
      "Epoch: 766 \tTraining Loss: 0.00000000030695 \tValidation Loss: 0.00000000001883\n",
      "Epoch: 767 \tTraining Loss: 0.00000000030687 \tValidation Loss: 0.00000000001842\n",
      "Epoch: 768 \tTraining Loss: 0.00000000030505 \tValidation Loss: 0.00000000001824\n",
      "Epoch: 769 \tTraining Loss: 0.00000000030467 \tValidation Loss: 0.00000000001827\n",
      "Epoch: 770 \tTraining Loss: 0.00000000030581 \tValidation Loss: 0.00000000001816\n",
      "Epoch: 771 \tTraining Loss: 0.00000000029840 \tValidation Loss: 0.00000000001919\n",
      "Epoch: 772 \tTraining Loss: 0.00000000030281 \tValidation Loss: 0.00000000001853\n",
      "Epoch: 773 \tTraining Loss: 0.00000000029942 \tValidation Loss: 0.00000000001894\n",
      "Epoch: 774 \tTraining Loss: 0.00000000030104 \tValidation Loss: 0.00000000001775\n",
      "Validation loss decreased (0.00000000001783 --> 0.00000000001775).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 775 \tTraining Loss: 0.00000000029571 \tValidation Loss: 0.00000000001820\n",
      "Epoch: 776 \tTraining Loss: 0.00000000029827 \tValidation Loss: 0.00000000001818\n",
      "Epoch: 777 \tTraining Loss: 0.00000000029532 \tValidation Loss: 0.00000000001809\n",
      "Epoch: 778 \tTraining Loss: 0.00000000029465 \tValidation Loss: 0.00000000001809\n",
      "Epoch: 779 \tTraining Loss: 0.00000000029406 \tValidation Loss: 0.00000000001814\n",
      "Epoch: 780 \tTraining Loss: 0.00000000029495 \tValidation Loss: 0.00000000001770\n",
      "Validation loss decreased (0.00000000001775 --> 0.00000000001770).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 781 \tTraining Loss: 0.00000000029074 \tValidation Loss: 0.00000000001762\n",
      "Validation loss decreased (0.00000000001770 --> 0.00000000001762).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 782 \tTraining Loss: 0.00000000029154 \tValidation Loss: 0.00000000001768\n",
      "Epoch: 783 \tTraining Loss: 0.00000000028884 \tValidation Loss: 0.00000000001799\n",
      "Epoch: 784 \tTraining Loss: 0.00000000028893 \tValidation Loss: 0.00000000001797\n",
      "Epoch: 785 \tTraining Loss: 0.00000000028979 \tValidation Loss: 0.00000000001810\n",
      "Epoch: 786 \tTraining Loss: 0.00000000028886 \tValidation Loss: 0.00000000001753\n",
      "Validation loss decreased (0.00000000001762 --> 0.00000000001753).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 787 \tTraining Loss: 0.00000000028681 \tValidation Loss: 0.00000000001792\n",
      "Epoch: 788 \tTraining Loss: 0.00000000028713 \tValidation Loss: 0.00000000001764\n",
      "Epoch: 789 \tTraining Loss: 0.00000000028485 \tValidation Loss: 0.00000000001824\n",
      "Epoch: 790 \tTraining Loss: 0.00000000028692 \tValidation Loss: 0.00000000001745\n",
      "Validation loss decreased (0.00000000001753 --> 0.00000000001745).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 791 \tTraining Loss: 0.00000000028389 \tValidation Loss: 0.00000000001669\n",
      "Validation loss decreased (0.00000000001745 --> 0.00000000001669).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 792 \tTraining Loss: 0.00000000028199 \tValidation Loss: 0.00000000001745\n",
      "Epoch: 793 \tTraining Loss: 0.00000000028286 \tValidation Loss: 0.00000000001715\n",
      "Epoch: 794 \tTraining Loss: 0.00000000028234 \tValidation Loss: 0.00000000001693\n",
      "Epoch: 795 \tTraining Loss: 0.00000000028111 \tValidation Loss: 0.00000000001667\n",
      "Validation loss decreased (0.00000000001669 --> 0.00000000001667).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 796 \tTraining Loss: 0.00000000027841 \tValidation Loss: 0.00000000001783\n",
      "Epoch: 797 \tTraining Loss: 0.00000000027999 \tValidation Loss: 0.00000000001796\n",
      "Epoch: 798 \tTraining Loss: 0.00000000027990 \tValidation Loss: 0.00000000001721\n",
      "Epoch: 799 \tTraining Loss: 0.00000000027563 \tValidation Loss: 0.00000000001756\n",
      "Epoch: 800 \tTraining Loss: 0.00000000027804 \tValidation Loss: 0.00000000001715\n",
      "Epoch: 801 \tTraining Loss: 0.00000000027707 \tValidation Loss: 0.00000000001710\n",
      "results/results_n=1.0\n",
      "90000\n",
      "Epoch: 802 \tTraining Loss: 0.00000000027645 \tValidation Loss: 0.00000000001721\n",
      "Epoch: 803 \tTraining Loss: 0.00000000027541 \tValidation Loss: 0.00000000001661\n",
      "Validation loss decreased (0.00000000001667 --> 0.00000000001661).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 804 \tTraining Loss: 0.00000000027383 \tValidation Loss: 0.00000000001699\n",
      "Epoch: 805 \tTraining Loss: 0.00000000027483 \tValidation Loss: 0.00000000001648\n",
      "Validation loss decreased (0.00000000001661 --> 0.00000000001648).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 806 \tTraining Loss: 0.00000000027224 \tValidation Loss: 0.00000000001729\n",
      "Epoch: 807 \tTraining Loss: 0.00000000027401 \tValidation Loss: 0.00000000001749\n",
      "Epoch: 808 \tTraining Loss: 0.00000000027111 \tValidation Loss: 0.00000000001691\n",
      "Epoch: 809 \tTraining Loss: 0.00000000027074 \tValidation Loss: 0.00000000001702\n",
      "Epoch: 810 \tTraining Loss: 0.00000000027101 \tValidation Loss: 0.00000000001667\n",
      "Epoch: 811 \tTraining Loss: 0.00000000026958 \tValidation Loss: 0.00000000001667\n",
      "Epoch: 812 \tTraining Loss: 0.00000000027066 \tValidation Loss: 0.00000000001650\n",
      "Epoch: 813 \tTraining Loss: 0.00000000026846 \tValidation Loss: 0.00000000001574\n",
      "Validation loss decreased (0.00000000001648 --> 0.00000000001574).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 814 \tTraining Loss: 0.00000000026757 \tValidation Loss: 0.00000000001661\n",
      "Epoch: 815 \tTraining Loss: 0.00000000026904 \tValidation Loss: 0.00000000001598\n",
      "Epoch: 816 \tTraining Loss: 0.00000000026744 \tValidation Loss: 0.00000000001589\n",
      "Epoch: 817 \tTraining Loss: 0.00000000026677 \tValidation Loss: 0.00000000001628\n",
      "Epoch: 818 \tTraining Loss: 0.00000000026584 \tValidation Loss: 0.00000000001658\n",
      "Epoch: 819 \tTraining Loss: 0.00000000026405 \tValidation Loss: 0.00000000001667\n",
      "Epoch: 820 \tTraining Loss: 0.00000000026589 \tValidation Loss: 0.00000000001600\n",
      "Epoch: 821 \tTraining Loss: 0.00000000026273 \tValidation Loss: 0.00000000001658\n",
      "Epoch: 822 \tTraining Loss: 0.00000000026433 \tValidation Loss: 0.00000000001574\n",
      "Validation loss decreased (0.00000000001574 --> 0.00000000001574).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 823 \tTraining Loss: 0.00000000026258 \tValidation Loss: 0.00000000001617\n",
      "Epoch: 824 \tTraining Loss: 0.00000000026170 \tValidation Loss: 0.00000000001671\n",
      "Epoch: 825 \tTraining Loss: 0.00000000026230 \tValidation Loss: 0.00000000001624\n",
      "Epoch: 826 \tTraining Loss: 0.00000000026029 \tValidation Loss: 0.00000000001643\n",
      "Epoch: 827 \tTraining Loss: 0.00000000025986 \tValidation Loss: 0.00000000001628\n",
      "Epoch: 828 \tTraining Loss: 0.00000000025857 \tValidation Loss: 0.00000000001637\n",
      "Epoch: 829 \tTraining Loss: 0.00000000026112 \tValidation Loss: 0.00000000001553\n",
      "Validation loss decreased (0.00000000001574 --> 0.00000000001553).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 830 \tTraining Loss: 0.00000000025848 \tValidation Loss: 0.00000000001533\n",
      "Validation loss decreased (0.00000000001553 --> 0.00000000001533).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 831 \tTraining Loss: 0.00000000025837 \tValidation Loss: 0.00000000001606\n",
      "Epoch: 832 \tTraining Loss: 0.00000000025477 \tValidation Loss: 0.00000000001650\n",
      "Epoch: 833 \tTraining Loss: 0.00000000025850 \tValidation Loss: 0.00000000001626\n",
      "Epoch: 834 \tTraining Loss: 0.00000000025505 \tValidation Loss: 0.00000000001593\n",
      "Epoch: 835 \tTraining Loss: 0.00000000025705 \tValidation Loss: 0.00000000001501\n",
      "Validation loss decreased (0.00000000001533 --> 0.00000000001501).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 836 \tTraining Loss: 0.00000000025542 \tValidation Loss: 0.00000000001509\n",
      "Epoch: 837 \tTraining Loss: 0.00000000025388 \tValidation Loss: 0.00000000001555\n",
      "Epoch: 838 \tTraining Loss: 0.00000000025323 \tValidation Loss: 0.00000000001566\n",
      "Epoch: 839 \tTraining Loss: 0.00000000025388 \tValidation Loss: 0.00000000001607\n",
      "Epoch: 840 \tTraining Loss: 0.00000000025153 \tValidation Loss: 0.00000000001667\n",
      "Epoch: 841 \tTraining Loss: 0.00000000025557 \tValidation Loss: 0.00000000001512\n",
      "Epoch: 842 \tTraining Loss: 0.00000000025230 \tValidation Loss: 0.00000000001486\n",
      "Validation loss decreased (0.00000000001501 --> 0.00000000001486).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 843 \tTraining Loss: 0.00000000025183 \tValidation Loss: 0.00000000001490\n",
      "Epoch: 844 \tTraining Loss: 0.00000000024907 \tValidation Loss: 0.00000000001613\n",
      "Epoch: 845 \tTraining Loss: 0.00000000025028 \tValidation Loss: 0.00000000001526\n",
      "Epoch: 846 \tTraining Loss: 0.00000000025032 \tValidation Loss: 0.00000000001507\n",
      "Epoch: 847 \tTraining Loss: 0.00000000024974 \tValidation Loss: 0.00000000001511\n",
      "Epoch: 848 \tTraining Loss: 0.00000000024932 \tValidation Loss: 0.00000000001496\n",
      "Epoch: 849 \tTraining Loss: 0.00000000024883 \tValidation Loss: 0.00000000001552\n",
      "Epoch: 850 \tTraining Loss: 0.00000000024932 \tValidation Loss: 0.00000000001477\n",
      "Validation loss decreased (0.00000000001486 --> 0.00000000001477).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 851 \tTraining Loss: 0.00000000024626 \tValidation Loss: 0.00000000001507\n",
      "Epoch: 852 \tTraining Loss: 0.00000000025053 \tValidation Loss: 0.00000000001552\n",
      "Epoch: 853 \tTraining Loss: 0.00000000024516 \tValidation Loss: 0.00000000001514\n",
      "Epoch: 854 \tTraining Loss: 0.00000000024518 \tValidation Loss: 0.00000000001507\n",
      "Epoch: 855 \tTraining Loss: 0.00000000024386 \tValidation Loss: 0.00000000001572\n",
      "Epoch: 856 \tTraining Loss: 0.00000000024505 \tValidation Loss: 0.00000000001511\n",
      "Epoch: 857 \tTraining Loss: 0.00000000024306 \tValidation Loss: 0.00000000001511\n",
      "Epoch: 858 \tTraining Loss: 0.00000000024609 \tValidation Loss: 0.00000000001477\n",
      "Validation loss decreased (0.00000000001477 --> 0.00000000001477).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 859 \tTraining Loss: 0.00000000024382 \tValidation Loss: 0.00000000001477\n",
      "Epoch: 860 \tTraining Loss: 0.00000000024142 \tValidation Loss: 0.00000000001529\n",
      "Epoch: 861 \tTraining Loss: 0.00000000024240 \tValidation Loss: 0.00000000001514\n",
      "Epoch: 862 \tTraining Loss: 0.00000000024090 \tValidation Loss: 0.00000000001552\n",
      "Epoch: 863 \tTraining Loss: 0.00000000024104 \tValidation Loss: 0.00000000001488\n",
      "Epoch: 864 \tTraining Loss: 0.00000000024131 \tValidation Loss: 0.00000000001481\n",
      "Epoch: 865 \tTraining Loss: 0.00000000023965 \tValidation Loss: 0.00000000001589\n",
      "Epoch: 866 \tTraining Loss: 0.00000000024132 \tValidation Loss: 0.00000000001499\n",
      "Epoch: 867 \tTraining Loss: 0.00000000024095 \tValidation Loss: 0.00000000001447\n",
      "Validation loss decreased (0.00000000001477 --> 0.00000000001447).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 868 \tTraining Loss: 0.00000000023909 \tValidation Loss: 0.00000000001507\n",
      "Epoch: 869 \tTraining Loss: 0.00000000023931 \tValidation Loss: 0.00000000001492\n",
      "Epoch: 870 \tTraining Loss: 0.00000000023751 \tValidation Loss: 0.00000000001511\n",
      "Epoch: 871 \tTraining Loss: 0.00000000024010 \tValidation Loss: 0.00000000001410\n",
      "Validation loss decreased (0.00000000001447 --> 0.00000000001410).  Saving model ...\n",
      " loss >= 0.00000\n",
      "Epoch: 872 \tTraining Loss: 0.00000000023747 \tValidation Loss: 0.00000000001429\n",
      "Epoch: 873 \tTraining Loss: 0.00000000023887 \tValidation Loss: 0.00000000001566\n",
      "Epoch: 874 \tTraining Loss: 0.00000000023888 \tValidation Loss: 0.00000000001462\n",
      "Epoch: 875 \tTraining Loss: 0.00000000023510 \tValidation Loss: 0.00000000001425\n",
      "Epoch: 876 \tTraining Loss: 0.00000000023521 \tValidation Loss: 0.00000000001485\n",
      "Epoch: 877 \tTraining Loss: 0.00000000023467 \tValidation Loss: 0.00000000001481\n",
      "Epoch: 878 \tTraining Loss: 0.00000000023413 \tValidation Loss: 0.00000000001477\n",
      "Epoch: 879 \tTraining Loss: 0.00019165484858 \tValidation Loss: 0.00000005734860\n",
      "Epoch: 880 \tTraining Loss: 0.00000067348178 \tValidation Loss: 0.00000003108899\n",
      "Epoch: 881 \tTraining Loss: 0.00000041685330 \tValidation Loss: 0.00000002255723\n",
      "Epoch: 882 \tTraining Loss: 0.00000030982935 \tValidation Loss: 0.00000001771812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Add()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madd.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_to_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(save_file, model, criterion, train_loader, valid_loader, optimizer, n_epochs, f, lrate)\u001b[0m\n\u001b[0;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# perform a single optimization step (parameter update)\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# update running training loss\u001b[39;00m\n\u001b[0;32m     38\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mX\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mrcgg\\anaconda3\\envs\\ML-pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\mrcgg\\anaconda3\\envs\\ML-pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\mrcgg\\anaconda3\\envs\\ML-pytorch\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\mrcgg\\anaconda3\\envs\\ML-pytorch\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mrcgg\\anaconda3\\envs\\ML-pytorch\\lib\\site-packages\\torch\\optim\\adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    412\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Add().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "train(\"add.pt\", model, criterion, train_loader, valid_loader, f=noise_to_int, lrate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Add().to(device)\n",
    "model.load_state_dict(torch.load(\"add.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.],\n",
      "       device='cuda:0') tensor([1.0000e+00, 1.0000e+00, 3.6383e-10, 1.5164e-11, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 7.4644e-11, 1.0000e+00, 1.0000e+00, 3.9853e-11, 1.1535e-10,\n",
      "        8.6950e-11, 1.0000e+00, 3.1158e-10, 1.0000e+00, 8.1699e-11, 2.5697e-10,\n",
      "        1.0000e+00, 1.3292e-11, 1.8079e-11, 6.9031e-11, 3.8012e-10, 1.0000e+00,\n",
      "        1.4691e-10, 2.0301e-10, 1.0000e+00, 6.0128e-11, 1.0000e+00, 6.4623e-11,\n",
      "        1.0000e+00, 5.9685e-11], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X, Y = next(iter(test_loader))\n",
    "O = model(X)\n",
    "print(Y[1], O[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/results_n=1.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "results = 0\n",
    "results_n = 0\n",
    "\n",
    "for X,Y in train_loader:\n",
    "    # print(X,Y)\n",
    "    with torch.no_grad():\n",
    "        O = model(X)\n",
    "    for x,y in zip(O,Y):\n",
    "        a = noise_to_int(x.cpu().detach().numpy())\n",
    "        b = noise_to_int(y.cpu().detach().numpy())\n",
    "        # b = int(y[0]) pos_to_int\n",
    "        # b = noise_to_int(y)\n",
    "        # print(x,y)\n",
    "        # print(a,b)\n",
    "        if a==b:\n",
    "            \n",
    "            results +=1\n",
    "        results_n +=1\n",
    "print(f\"{results/results_n=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/results_n=1.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "results = 0\n",
    "results_n = 0\n",
    "noise = 0.01\n",
    "for i in range(10000):\n",
    "    a = torch.randint(0, 2**32, (1,)).item()\n",
    "    b = torch.randint(0, 2**32, (1,)).item()\n",
    "    A = torch.tensor([float(a) for a in bin(a)[2:].rjust(32,\"0\")], dtype=torch.float32)\n",
    "    A += (torch.FloatTensor(32).uniform_(-noise, +noise))\n",
    "    B = torch.tensor([float(a) for a in bin(b)[2:].rjust(32,\"0\")], dtype=torch.float32)\n",
    "    B += (torch.FloatTensor(32).uniform_(-noise, +noise))\n",
    "    \n",
    "    X = torch.zeros(32*2, dtype=torch.float32)\n",
    "    X[0::2] = A\n",
    "    X[1::2] = B\n",
    "    X = X.view(32,2)\n",
    "    X = X.flip(0)\n",
    "    X = X.to(device)\n",
    "    X = X.unsqueeze(0)\n",
    "\n",
    "    Y = torch.tensor([float(a) for a in bin(add(a,b))[2:].rjust(32,\"0\")], dtype=torch.float32).to(device)\n",
    "    Y = Y.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        O = model(X)\n",
    "    for x,y in zip(O,Y):\n",
    "        a = noise_to_int(x.cpu().detach().numpy())\n",
    "        b = noise_to_int(y.cpu().detach().numpy())\n",
    "        # b = int(y[0]) pos_to_int\n",
    "        # b = noise_to_int(y)\n",
    "        # print(x,y)\n",
    "        # print(a,b)\n",
    "        if a==b:\n",
    "            \n",
    "            results +=1\n",
    "        else:\n",
    "            print(a,b)\n",
    "        results_n +=1\n",
    "print(f\"{results/results_n=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm\n",
      "torch.Size([8, 2])\n",
      "torch.Size([8, 2])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([[-6.2521,  5.9953],\n",
      "        [ 4.6506,  1.7229],\n",
      "        [-1.6022, -2.4458],\n",
      "        [ 2.7108,  4.5105],\n",
      "        [-6.3193,  2.4689],\n",
      "        [ 0.5226, -7.1532],\n",
      "        [-5.5024,  5.5650],\n",
      "        [ 4.7941,  5.5716]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.0304,  8.0028],\n",
      "        [-0.1866, -3.8193],\n",
      "        [-1.3315,  6.0119],\n",
      "        [-1.7743, -0.6462],\n",
      "        [ 0.2903,  3.2302],\n",
      "        [ 0.3963,  4.5638],\n",
      "        [ 0.8516,  8.9225],\n",
      "        [ 0.1657,  1.7805]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.6164, -2.5298, -0.7971, -1.9898, -1.2576,  0.0660,  1.2347, -1.5695],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.5969, -3.2419, -0.4246, -2.5121, -0.9013, -0.5101,  1.7107, -1.5696],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "2\n",
      "2\n",
      "2\n",
      "True\n",
      "0.0\n",
      "False\n",
      "True\n",
      "0\n",
      "LSTM(2, 2, num_layers=2, batch_first=True)\n",
      "\n",
      "linear\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([[ 2.0042e+00,  1.1554e+00,  1.1770e+00,  ..., -2.4635e+00,\n",
      "         -2.9520e+01,  1.9608e+01],\n",
      "        [ 1.8301e+00,  1.1023e+00,  1.4263e+00,  ...,  1.9568e+01,\n",
      "          6.6796e+00,  5.7156e+00],\n",
      "        [ 1.5764e+00,  1.0089e+00,  1.1332e+00,  ...,  5.3553e+00,\n",
      "          3.1530e+00,  2.2710e+00],\n",
      "        ...,\n",
      "        [ 1.0103e+00, -1.2357e-02,  3.1789e+00,  ...,  2.7925e-01,\n",
      "          7.6283e-01,  5.9508e-01],\n",
      "        [ 5.5063e+00, -3.5183e+00, -2.5610e+01,  ...,  2.4523e-01,\n",
      "          4.9500e-01,  3.8326e-01],\n",
      "        [-1.9139e+01,  1.5938e+01,  3.7184e+00,  ...,  2.6557e-01,\n",
      "          2.4980e-01,  3.0689e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([10.3791,  9.5241,  8.7802,  8.8404,  8.1060,  7.5920,  7.2376,  7.3295,\n",
      "         7.1370,  7.0378,  7.0009,  7.2007,  6.9304,  7.1514,  7.1819,  7.1563,\n",
      "         6.9560,  7.0116,  7.3177,  7.1974,  7.5214,  7.1765,  6.8717,  6.8163,\n",
      "         6.9062,  6.6950,  6.4478,  7.1455,  7.3702,  7.4602,  5.2998,  2.0751],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in model.children():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        print(\"linear\")\n",
    "        print(layer.weight.shape)\n",
    "        print(layer.bias.shape)\n",
    "        print(layer.weight)\n",
    "        print(layer.bias)\n",
    "    if isinstance(layer, nn.LSTM):\n",
    "        print(\"lstm\")\n",
    "        print(layer.weight_ih_l0.shape)\n",
    "        print(layer.weight_hh_l0.shape)\n",
    "        print(layer.bias_ih_l0.shape)\n",
    "        print(layer.bias_hh_l0.shape)\n",
    "        print(layer.weight_ih_l0)\n",
    "        print(layer.weight_hh_l0)\n",
    "        print(layer.bias_ih_l0)\n",
    "        print(layer.bias_hh_l0)\n",
    "        print(layer.hidden_size)\n",
    "        print(layer.input_size)\n",
    "        print(layer.num_layers)\n",
    "        print(layer.bias)\n",
    "        print(layer.dropout)\n",
    "        print(layer.bidirectional)\n",
    "        print(layer.batch_first)\n",
    "        print(layer.proj_size)\n",
    "        print(layer)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.,  10.], device='cuda:0')\n",
      "tensor([-100.,   10.], device='cuda:0')\n",
      "tensor([ 10., -10.], device='cuda:0')\n",
      "tensor([100., -10.], device='cuda:0')\n",
      "tensor(-50., device='cuda:0')\n",
      "tensor(-50., device='cuda:0')\n",
      "tensor([20., 20.], device='cuda:0')\n",
      "tensor([200.,  20.], device='cuda:0')\n",
      "tensor(-100., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "state = torch.load(\"xor.pt\")\n",
    "\n",
    "for w in state[\"body.0.weight\"]:\n",
    "    for i in range(len(w)):\n",
    "        print(w)\n",
    "        if w[i] > 0:\n",
    "            w[i] = 100.0\n",
    "        else:\n",
    "            w[i] = -100.0\n",
    "\n",
    "for i in range(len(state[\"body.0.bias\"])):\n",
    "    state[\"body.0.bias\"][i] = -50.0\n",
    "    print(state[\"body.0.bias\"][i])\n",
    "\n",
    "for w in state[\"body.2.weight\"]:\n",
    "    for i in range(len(w)):\n",
    "        print(w)\n",
    "        if w[i] > 0:\n",
    "            w[i] = 200.0\n",
    "        else:\n",
    "            w[i] = 0.0\n",
    "\n",
    "for i in range(len(state[\"body.2.bias\"])):\n",
    "    state[\"body.2.bias\"][i] = -100.0\n",
    "    print(state[\"body.2.bias\"][i])\n",
    "torch.save(state,\"xor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c4a0582a2f0c696ca3bf144d7a797081d9cfe83a50d52b918de14dcec4b2aff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
